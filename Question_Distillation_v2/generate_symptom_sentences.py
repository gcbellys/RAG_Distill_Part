import os
import sys
import argparse
import json
from loguru import logger
from tqdm import tqdm

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(script_dir, "../.."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from RAG_Evidence4Organ.knowledge_distillation.extractors.llm_extractor import create_extractor
from RAG_Evidence4Organ.configs.system_config import MULTI_API_CONFIG, PROJECT_ROOT
from RAG_Evidence4Organ.configs.model_config import ALLOWED_ORGANS
from RAG_Evidence4Organ.prompts.medical_prompts import get_symptom_sentence_extraction_prompt, get_organ_classification_prompt

def detect_organ_from_text(text: str) -> str:
    """
    æ ¹æ®å…³é”®è¯ä»æ–‡æœ¬ä¸­æ£€æµ‹ä¸»è¦å™¨å®˜ã€‚
    ä½¿ç”¨ ORGAN_MAPPING ä»¥è·å¾—æ›´å¹¿æ³›çš„å…³é”®è¯åŒ¹é…ã€‚
    """
    text_lower = text.lower()
    # ä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„å…³é”®è¯ï¼Œæ‰€ä»¥å¯ä»¥æŒ‰é”®é•¿æ’åº
    # (è™½ç„¶åœ¨å½“å‰åœºæ™¯ä¸‹å½±å“ä¸å¤§ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯)
    sorted_keywords = sorted(ORGAN_MAPPING.keys(), key=len, reverse=True)
    
    for keyword in sorted_keywords:
        if keyword in text_lower:
            return ORGAN_MAPPING[keyword]
            
    return "Unknown"

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ä»åŒ»ç–—æŠ¥å‘Šä¸­æå–ç—‡çŠ¶/å‘ç°çš„å®Œæ•´å¥å­å—ã€‚")
    parser.add_argument("--start", type=int, required=True, help="èµ·å§‹æŠ¥å‘Šæ–‡ä»¶ç¼–å·ã€‚")
    parser.add_argument("--end", type=int, required=True, help="ç»“æŸæŠ¥å‘Šæ–‡ä»¶ç¼–å·ï¼ˆåŒ…å«ï¼‰ã€‚")
    parser.add_argument("--api_key_name", type=str, default="api_1", help="åœ¨system_configä¸­è¦ä½¿ç”¨çš„APIå¯†é’¥åç§°ã€‚")
    parser.add_argument("--output_file", type=str, default="symptom_sentence_corpus_test.json", help="è¾“å‡ºJSONæ–‡ä»¶çš„åç§°ã€‚")
    return parser.parse_args()

def main():
    args = parse_args()
    logger.info(f"ğŸš€ å¼€å§‹æå–ç—‡çŠ¶å¥å­ï¼ŒèŒƒå›´: {args.start}-{args.end}")

    # è·å–APIé…ç½®
    if args.api_key_name not in MULTI_API_CONFIG:
        logger.error(f"é”™è¯¯: APIå¯†é’¥ '{args.api_key_name}' åœ¨ system_config.py ä¸­æœªæ‰¾åˆ°ã€‚")
        return
    api_config = MULTI_API_CONFIG[args.api_key_name]

    # åˆå§‹åŒ–æå–å™¨
    try:
        extractor = create_extractor(
            model=api_config["model"],
            api_key=api_config["api_key"],
            base_url=api_config["base_url"]
        )
        logger.success(f"LLMæå–å™¨åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {api_config['model']}")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–LLMæå–å™¨å¤±è´¥: {e}")
        return
        
    dataset_dir = PROJECT_ROOT / "RAG_Evidence4Organ" / "dataset"
    all_results = []

    file_range = range(args.start, args.end + 1)
    for report_num in tqdm(file_range, desc="å¤„ç†æŠ¥å‘Š"):
        report_filename = f"report_{report_num}.txt"
        report_path = dataset_dir / report_filename
        case_id = str(report_num)

        if not report_path.exists():
            logger.warning(f"æ–‡ä»¶ {report_filename} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue

        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                report_text = f.read()
            
            if not report_text.strip():
                logger.warning(f"æ–‡ä»¶ {report_filename} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            # --- STAGE 1: Classify Organ System ---
            classification_prompt = get_organ_classification_prompt(report_text)
            classification_response = extractor.call_api(classification_prompt)

            if not classification_response or not classification_response.get("success"):
                logger.error(f"æŠ¥å‘Š {case_id}: å™¨å®˜åˆ†ç±»APIè°ƒç”¨å¤±è´¥ã€‚è·³è¿‡ã€‚")
                continue
            
            primary_organ = classification_response["response"].strip().replace('"', '')

            if primary_organ not in ALLOWED_ORGANS:
                logger.info(f"æŠ¥å‘Š {case_id}: ä¸»è¦å™¨å®˜ä¸º '{primary_organ}'ï¼Œä¸å±äºç›®æ ‡å™¨å®˜ï¼Œè·³è¿‡ã€‚")
                continue

            logger.success(f"æŠ¥å‘Š {case_id}: å·²è¯†åˆ«ä¸ºä¸ '{primary_organ}' ç›¸å…³ï¼Œå¼€å§‹æå–ç—‡çŠ¶å¥å­ã€‚")

            # --- STAGE 2: Extract Sentences using chunking ---
            chunk_num = 1
            report_fully_extracted = False
            while not report_fully_extracted:
                prompt = get_symptom_sentence_extraction_prompt(report_text, chunk_num=chunk_num)
                api_response = extractor.call_api(prompt)

                if not (api_response and api_response.get("success")):
                    logger.error(f"æŠ¥å‘Š {case_id} (å— {chunk_num}): APIè°ƒç”¨å¤±è´¥ã€‚")
                    break # Stop processing this report

                response_text = api_response["response"].strip()
                
                # Cleanup logic
                if response_text.startswith("```json"):
                    response_text = response_text[7:].strip()
                if response_text.startswith("```"):
                    response_text = response_text[3:].strip()
                if response_text.endswith("```"):
                    response_text = response_text[:-3].strip()

                try:
                    extracted_blocks = json.loads(response_text)
                    
                    if not isinstance(extracted_blocks, list):
                        logger.warning(f"æŠ¥å‘Š {case_id} (å— {chunk_num}): LLMè¿”å›äº†éåˆ—è¡¨æ ¼å¼çš„JSONï¼Œåœæ­¢å¤„ç†æ­¤æŠ¥å‘Šã€‚")
                        break

                    if not extracted_blocks: # Empty list means we are done
                        report_fully_extracted = True
                        logger.info(f"æŠ¥å‘Š {case_id}: LLMåœ¨å— {chunk_num} è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæå–å®Œæˆã€‚")
                        continue

                    # Process the received chunk
                    for block in extracted_blocks:
                        if "original_sentences" in block and isinstance(block["original_sentences"], list) and block["original_sentences"]:
                            block['case_id'] = case_id
                            block['inferred_organ'] = primary_organ
                            all_results.append(block)
                    
                    logger.info(f"æŠ¥å‘Š {case_id}: æˆåŠŸå¤„ç†å— {chunk_num}ï¼Œæå–äº† {len(extracted_blocks)} æ¡æ–°å¥å­å—ã€‚")
                    chunk_num += 1

                except json.JSONDecodeError:
                    logger.error(f"æŠ¥å‘Š {case_id} (å— {chunk_num}): æ— æ³•è§£æLLMè¿”å›çš„JSONå“åº”ã€‚å°†å°è¯•è¯·æ±‚ä¸‹ä¸€å—ã€‚")
                    logger.debug(f"åŸå§‹å“åº”: {response_text}") # Log the faulty response for debugging
                    chunk_num += 1 # Crucial: increment chunk_num to avoid an infinite loop on the same faulty chunk
                    continue # Skip to the next iteration of the while loop

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {report_filename} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    output_path = PROJECT_ROOT / "RAG_Evidence4Organ" / args.output_file
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        logger.success(f"âœ… æå–å®Œæˆï¼æ€»å…±æå–äº† {len(all_results)} æ¡å¥å­å—ã€‚")
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    except IOError as e:
        logger.error(f"æ— æ³•å†™å…¥è¾“å‡ºæ–‡ä»¶ {output_path}: {e}")

if __name__ == "__main__":
    main() 