#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œè’¸é¦å·¥ä½œè„šæœ¬ (Worker)
ä½œè€…: Gemini & CDJ_LP
æè¿°: 
è¯¥è„šæœ¬æ˜¯å¹¶è¡Œå¤„ç†æµç¨‹çš„æ ¸å¿ƒå·¥ä½œå•å…ƒã€‚å®ƒæ¥æ”¶ä¸€ä¸ªæ–‡ä»¶èŒƒå›´å’Œ
ä¸€ä¸ªAPIé…ç½®åç§°ï¼Œå¤„ç†æŒ‡å®šèŒƒå›´å†…çš„æŠ¥å‘Šï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°ç‹¬ç«‹æ–‡ä»¶ã€‚

æ‰§è¡Œæ–¹æ³• (ç”±å¯åŠ¨è„šæœ¬è°ƒç”¨):
python RAG_Evidence4Organ/Question_Distillation_v2/process_worker.py \
    --input_dir RAG_Evidence4Organ/dataset/ \
    --output_dir RAG_Evidence4Organ/Question_Distillation_v2/results/ \
    --api_key_name api_1 \
    --start_index 0 \
    --end_index 100
"""

import os
import sys
import json
import argparse
import re
from typing import List, Dict, Any
from loguru import logger
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = "/opt/RAG_Evidence4Organ"
sys.path.insert(0, project_root)

from Question_Distillation_v2.extractors.llm_extractor import LLMExtractor
from Question_Distillation_v2.prompts.medical_prompts import MedicalExtractionPrompts
from configs.system_config import MULTI_API_CONFIG
from configs.model_config import ORGAN_ANATOMY_STRUCTURE

def numeric_sort_key(s: str):
    """ä¸ºæ•°å­—æ’åºç”Ÿæˆkey, e.g., 'report_1.txt' < 'report_2.txt' < 'report_10.txt'"""
    # æå–æ–‡ä»¶åä¸­çš„æ•°å­—éƒ¨åˆ†
    match = re.search(r'report_(\d+)\.txt', s)
    if match:
        return int(match.group(1))
    return 0  # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ•°å­—ï¼Œè¿”å›0

def load_reports_from_list(file_path: str) -> List[Dict[str, Any]]:
    """ä»ä¸€ä¸ªæ–‡ä»¶åˆ—è¡¨æ–‡ä»¶ä¸­åŠ è½½æŠ¥å‘Š"""
    logger.info(f"æ­£åœ¨ä»ä»»åŠ¡åˆ—è¡¨ {file_path} åŠ è½½æŠ¥å‘Š...")
    reports = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            filepaths = [line.strip() for line in f if line.strip()]
        
        for report_path in filepaths:
            try:
                with open(report_path, 'r', encoding='utf-8') as report_file:
                    text = report_file.read()
                filename = os.path.basename(report_path)
                case_id_match = re.search(r'(\d+)', filename)
                case_id = case_id_match.group(1) if case_id_match else filename.replace('.txt', '')
                if text.strip():
                    reports.append({"case_id": case_id, "text": text, "filename": filename})
            except Exception as e:
                logger.error(f"è¯»å–æŠ¥å‘Šæ–‡ä»¶ {report_path} å¤±è´¥: {e}")
        
        logger.success(f"æˆåŠŸä»ä»»åŠ¡åˆ—è¡¨åŠ è½½ {len(reports)} æ¡æŠ¥å‘Šç”¨äºå¤„ç†ã€‚")
        return reports
    except Exception as e:
        logger.error(f"åŠ è½½ä»»åŠ¡åˆ—è¡¨æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def load_reports_in_range(directory_path: str, start_index: int, end_index: int) -> List[Dict[str, Any]]:
    """ä»ç›®å½•åŠ è½½æŒ‡å®šèŒƒå›´å†…çš„.txtæŠ¥å‘Š (ä½¿ç”¨è‡ªç„¶æ’åº)"""
    logger.info(f"æ­£åœ¨ä»ç›®å½• {directory_path} åŠ è½½ç´¢å¼•èŒƒå›´ {start_index}-{end_index} çš„æŠ¥å‘Š...")
    reports = []
    try:
        all_files = sorted(
            [f for f in os.listdir(directory_path) if f.endswith(".txt")],
            key=numeric_sort_key
        )
        files_in_range = all_files[start_index:end_index]

        for filename in files_in_range:
            file_path = os.path.join(directory_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                case_id_match = re.search(r'(\d+)', filename)
                case_id = case_id_match.group(1) if case_id_match else filename.replace('.txt', '')
                if text.strip():
                    reports.append({"case_id": case_id, "text": text, "filename": filename})
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        logger.success(f"æˆåŠŸåŠ è½½ {len(reports)} æ¡æŠ¥å‘Šç”¨äºå¤„ç†ã€‚")
        return reports
    except Exception as e:
        logger.error(f"åŠ è½½ç›®å½• {directory_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def validate_extraction(extraction: Dict[str, Any]) -> bool:
    """éªŒè¯å•ä¸ªæå–ç»“æœçš„åˆè§„æ€§ - æ”¯æŒæ‰€æœ‰å™¨å®˜"""
    required_keys = ["symptom_or_disease", "inferred_organ", "suggested_anatomical_parts_to_examine", "evidence_from_report"]
    if not all(key in extraction for key in required_keys): 
        return False
    
    organ = extraction["inferred_organ"]
    parts = extraction["suggested_anatomical_parts_to_examine"]
    
    # æ£€æŸ¥åŸºæœ¬è¦æ±‚
    if not isinstance(parts, list) or len(parts) < 3: 
        return False
    
    # æ£€æŸ¥å™¨å®˜ç±»åˆ«
    if organ in ORGAN_ANATOMY_STRUCTURE:
        # å¯¹äº5ä¸ªæŒ‡å®šå™¨å®˜ï¼ŒéªŒè¯è§£å‰–éƒ¨ä½æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­
        allowed_parts = ORGAN_ANATOMY_STRUCTURE[organ]
        if not all(part in allowed_parts for part in parts):
            return False
        # æ·»åŠ å™¨å®˜ç±»åˆ«æ ‡è¯†
        extraction["organ_category"] = "specified"
    else:
        # å¯¹äºå…¶ä»–å™¨å®˜ï¼ŒåªéªŒè¯åŸºæœ¬æ ¼å¼ï¼Œå…è®¸LLMè‡ªç”±æ¨æ–­
        # ç¡®ä¿è§£å‰–éƒ¨ä½ä¸ä¸ºç©ºä¸”æœ‰æ„ä¹‰
        if not all(part and len(part.strip()) > 0 for part in parts):
            return False
        # æ·»åŠ å™¨å®˜ç±»åˆ«æ ‡è¯†
        extraction["organ_category"] = "other"
    
    return True

def smart_chunk_medical_report(text: str) -> List[Dict[str, str]]:
    """
    åŸºäºåŒ»å­¦æŠ¥å‘Šç»“æ„è¿›è¡Œæ™ºèƒ½åˆ†å—
    è¿”å›: [{"section": "ç« èŠ‚å", "content": "å†…å®¹"}, ...]
    """
    import re
    
    # åŒ»å­¦æŠ¥å‘Šä¸­å¯èƒ½åŒ…å«ç—‡çŠ¶ä¿¡æ¯çš„å…³é”®ç« èŠ‚ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼é¿å…é‡å¤åŒ¹é…
    priority_section_patterns = [
        (r'brief hospital course:?', 'brief hospital course'),
        (r'hospital course by issue/system:?', 'hospital course by system'),  
        (r'hospital course by system:?', 'hospital course by system'),
        (r'hospital course:?', 'hospital course'),  # æ”¾åœ¨åé¢é¿å…è¢«briefè¦†ç›–
        (r'assessment and plan:?', 'assessment and plan'),
        (r'impression:?', 'impression'),
        (r'history of present illness:?', 'history of present illness'),
        (r'chief complaint:?', 'chief complaint'),
        (r'physical examination:?', 'physical examination'),  # ç»Ÿä¸€ä¸ºexamination
        (r'physical exam:?', 'physical examination'),  # æ˜ å°„åˆ°åŒä¸€ä¸ªåç§°
        (r'pertinent results:?', 'pertinent results'),
        (r'past medical history:?', 'past medical history')
    ]
    
    chunks = []
    text_lower = text.lower()
    used_sections = set()  # é˜²é‡å¤
    processed_ranges = []  # è®°å½•å·²å¤„ç†çš„æ–‡æœ¬èŒƒå›´ï¼Œé¿å…é‡å¤å†…å®¹
    
    # æ‰¾åˆ°æ‰€æœ‰å…³é”®ç« èŠ‚
    for pattern, canonical_name in priority_section_patterns:
        if canonical_name in used_sections:
            continue  # è·³è¿‡å·²å¤„ç†çš„ç« èŠ‚ç±»å‹
            
        matches = list(re.finditer(pattern, text_lower))
        for match in matches:
            start_pos = match.start()
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„èŒƒå›´é‡å 
            is_overlapping = any(
                abs(start_pos - existing_start) < 500  # å¦‚æœå¼€å§‹ä½ç½®å¾ˆæ¥è¿‘ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                for existing_start, _ in processed_ranges
            )
            if is_overlapping:
                logger.info(f"è·³è¿‡é‡å¤ç« èŠ‚ '{canonical_name}': ä¸å·²å¤„ç†å†…å®¹é‡å ")
                continue
            
            # æ‰¾åˆ°ç« èŠ‚ç»“æŸä½ç½®
            end_pos = len(text)
            for next_pattern, _ in priority_section_patterns:
                next_matches = list(re.finditer(next_pattern, text_lower[start_pos + len(match.group()):]))
                if next_matches:
                    potential_end = start_pos + len(match.group()) + next_matches[0].start()
                    if potential_end > start_pos and potential_end < end_pos:
                        end_pos = potential_end
            
            section_content = text[start_pos:end_pos].strip()
            
            # è¿‡æ»¤å¤ªçŸ­çš„ç« èŠ‚
            if len(section_content) < 200:
                continue
            
            # æ£€æŸ¥ç« èŠ‚æ˜¯å¦åŒ…å«ç—‡çŠ¶å…³é”®è¯
            symptom_keywords = [
                'pain', 'ache', 'fever', 'nausea', 'vomiting', 'bleeding', 'swelling',
                'dysfunction', 'failure', 'disease', 'syndrome', 'infection', 'inflammation',
                'hypertension', 'hypotension', 'tachycardia', 'bradycardia', 'arrhythmia',
                'pneumonia', 'embolism', 'infarction', 'ischemia', 'effusion', 'mass',
                'elevated', 'decreased', 'abnormal', 'positive', 'negative', 'acute', 'chronic'
            ]
            
            content_lower = section_content.lower()
            has_symptoms = any(keyword in content_lower for keyword in symptom_keywords)
            
            if not has_symptoms:
                logger.info(f"è·³è¿‡ç« èŠ‚ '{canonical_name}': æœªæ£€æµ‹åˆ°ç—‡çŠ¶å…³é”®è¯")
                continue
            
            # ä¼˜åŒ–åˆ†å—ç­–ç•¥ï¼šå¢åŠ å—å¤§å°ï¼Œå‡å°‘åˆ†å—æ•°é‡
            if len(section_content) > 4000:  # æé«˜é˜ˆå€¼
                sub_chunks = split_large_section_optimized(section_content, canonical_name)
                chunks.extend(sub_chunks)
            else:
                chunks.append({
                    "section": canonical_name,
                    "content": section_content
                })
            
            used_sections.add(canonical_name)
            processed_ranges.append((start_pos, end_pos))
            logger.info(f"è¯†åˆ«åˆ°æœ‰æ•ˆç« èŠ‚: '{canonical_name}' ({len(section_content)} å­—ç¬¦)")
            break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±è·³å‡ºï¼Œé¿å…é‡å¤
    
    if not chunks:
        logger.warning("æœªæ‰¾åˆ°æ ‡å‡†ç« èŠ‚ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†å—ç­–ç•¥")
        return fallback_smart_chunking(text)
    
    logger.info(f"ç« èŠ‚è¯†åˆ«å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæœ‰æ•ˆå—ï¼Œå·²å»é‡")
    return chunks

def fallback_smart_chunking(text: str) -> List[Dict[str, str]]:
    """å¤‡ç”¨çš„æ™ºèƒ½åˆ†å—æ–¹æ³• - åŸºäºå†…å®¹è€Œéç« èŠ‚"""
    import re
    
    # å¯»æ‰¾åŒ…å«ç—‡çŠ¶ä¿¡æ¯çš„æ®µè½
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""
    chunk_count = 1
    
    symptom_keywords = [
        'pain', 'ache', 'fever', 'nausea', 'bleeding', 'dysfunction', 'failure',
        'hypertension', 'tachycardia', 'pneumonia', 'elevated', 'decreased'
    ]
    
    for paragraph in paragraphs:
        paragraph_lower = paragraph.lower()
        has_symptoms = any(keyword in paragraph_lower for keyword in symptom_keywords)
        
        if has_symptoms or len(current_chunk) == 0:  # åŒ…å«ç—‡çŠ¶æˆ–æ˜¯ç¬¬ä¸€ä¸ªæ®µè½
            if len(current_chunk + paragraph) > 2500:
                if current_chunk:
                    chunks.append({
                        "section": f"content_block_{chunk_count}",
                        "content": current_chunk.strip()
                    })
                    chunk_count += 1
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    if current_chunk:
        chunks.append({
            "section": f"content_block_{chunk_count}",
            "content": current_chunk.strip()
        })
    
    return chunks

def split_large_section_optimized(content: str, section_name: str) -> List[Dict[str, str]]:
    """ä¼˜åŒ–çš„å¤§ç« èŠ‚åˆ†å‰² - å‡å°‘åˆ†å—æ•°é‡"""
    import re
    
    # å¢åŠ æ¯å—çš„å¤§å°é™åˆ¶ï¼Œå‡å°‘åˆ†å—æ•°é‡
    max_chunk_size = 3000  # ä»2000å¢åŠ åˆ°3000
    
    # å°è¯•æŒ‰æ•°å­—ç¼–å·åˆ†å‰² (å¦‚ 1., 2., 3.)
    numbered_items = re.split(r'\n\s*\d+\.', content)
    
    if len(numbered_items) > 1:
        chunks = []
        current_chunk = ""
        part_num = 1
        
        for i, item in enumerate(numbered_items):
            if not item.strip():
                continue
                
            if len(current_chunk + item) > max_chunk_size and current_chunk:
                chunks.append({
                    "section": f"{section_name} - part {part_num}",
                    "content": current_chunk.strip()
                })
                part_num += 1
                current_chunk = item.strip()
            else:
                current_chunk += "\n" + item.strip() if current_chunk else item.strip()
        
        if current_chunk:
            chunks.append({
                "section": f"{section_name} - part {part_num}",
                "content": current_chunk.strip()
            })
        
        return chunks
    
    # å¦‚æœæ²¡æœ‰æ•°å­—ç¼–å·ï¼ŒæŒ‰å¥å­åˆ†å‰²ï¼Œä½†ä½¿ç”¨æ›´å¤§çš„å—
    sentences = content.split('. ')
    chunks = []
    current_chunk = ""
    chunk_count = 1
    
    for sentence in sentences:
        if len(current_chunk + sentence + '. ') > max_chunk_size and current_chunk:
            chunks.append({
                "section": f"{section_name} - part {chunk_count}",
                "content": current_chunk.strip()
            })
            chunk_count += 1
            current_chunk = sentence + '. '
        else:
            current_chunk += sentence + '. '
    
    if current_chunk:
        chunks.append({
            "section": f"{section_name} - part {chunk_count}",
            "content": current_chunk.strip()
        })
    
    return chunks

def process_report(report: Dict[str, Any], extractor: LLMExtractor, prompt: str) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ä¼˜åŒ–çš„ç»“æ„åŒ–åˆ†å—å¤„ç†å•ä¸ªæŠ¥å‘Š"""
    text, case_id = report.get("text", ""), report.get("case_id", "N/A")
    logger.info(f"Workeræ­£åœ¨å¤„ç†ç—…ä¾‹: {case_id}...")
    
    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™é‡‡ç”¨ç»“æ„åŒ–åˆ†å—
    if len(text) > 4000:  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘ä¸å¿…è¦çš„åˆ†å—
        logger.info(f"ç—…ä¾‹ {case_id} æ–‡æœ¬è¾ƒé•¿({len(text)}å­—ç¬¦)ï¼Œé‡‡ç”¨ç»“æ„åŒ–åˆ†å—å¤„ç†")
        text_chunks = smart_chunk_medical_report(text)
        
        if not text_chunks:
            logger.warning(f"ç—…ä¾‹ {case_id} æœªèƒ½è¯†åˆ«åˆ°æœ‰æ•ˆç« èŠ‚ï¼Œè·³è¿‡å¤„ç†")
            return []
        
        logger.info(f"ç—…ä¾‹ {case_id} æœ€ç»ˆåˆ†å‰²ä¸º {len(text_chunks)} ä¸ªä¼˜åŒ–åçš„ç« èŠ‚å—")
        
        all_extractions = []
        successful_chunks = 0
        
        for i, chunk_info in enumerate(text_chunks):
            section_name = chunk_info["section"]
            chunk_content = chunk_info["content"]
            
            logger.info(f"å¤„ç†ç—…ä¾‹ {case_id} çš„ç« èŠ‚ {i+1}/{len(text_chunks)}: {section_name}")
            
            # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            enhanced_prompt = prompt + f"\n\n--- Medical Report Section: {section_name} ---\n" + chunk_content
            result = extractor.call_api(enhanced_prompt)
            
            if not (result and result.get("success")):
                logger.warning(f"ç—…ä¾‹ {case_id} ç« èŠ‚ '{section_name}' çš„LLMè°ƒç”¨å¤±è´¥")
                continue
            
            chunk_extractions = parse_llm_response(result.get("response", "[]"), case_id, section_name)
            if chunk_extractions:
                all_extractions.extend(chunk_extractions)
                successful_chunks += 1
        
        logger.success(f"ç—…ä¾‹ {case_id} ç»“æ„åŒ–åˆ†å—å¤„ç†å®Œæˆ: {successful_chunks}/{len(text_chunks)} ä¸ªç« èŠ‚æˆåŠŸï¼Œå…±æå– {len(all_extractions)} æ¡è®°å½•")
        return all_extractions
    else:
        # çŸ­æ–‡æœ¬çš„å•æ¬¡å¤„ç†é€»è¾‘
        logger.info(f"ç—…ä¾‹ {case_id} æ–‡æœ¬è¾ƒçŸ­({len(text)}å­—ç¬¦)ï¼Œé‡‡ç”¨å•æ¬¡å¤„ç†")
        full_prompt = prompt + "\n\n--- Medical Report ---\n" + text
        result = extractor.call_api(full_prompt)

        if not (result and result.get("success")):
            logger.warning(f"ç—…ä¾‹ {case_id} çš„LLMè°ƒç”¨å¤±è´¥ã€‚")
            return []
        
        extractions = parse_llm_response(result.get("response", "[]"), case_id)
        logger.success(f"ç—…ä¾‹ {case_id} å•æ¬¡å¤„ç†å®Œæˆï¼Œæå– {len(extractions)} æ¡è®°å½•")
        return extractions

def parse_llm_response(response_text: str, case_id: str, section_name: str = None) -> List[Dict[str, Any]]:
    """è§£æLLMå“åº”çš„ç‹¬ç«‹å‡½æ•°"""
    try:
        # æ‰“å°åŸå§‹LLMè¿”å›å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
        section_info = f" ç« èŠ‚[{section_name}]" if section_name else ""
        logger.info(f"ç—…ä¾‹ {case_id}{section_info} çš„LLMå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
        
        # å°è¯•ä¿®å¤è¢«æˆªæ–­çš„JSON
        # 1. é¦–å…ˆå°è¯•æ ‡å‡†è§£æ
        import re
        match = re.search(r'\[.*\]', response_text, re.DOTALL)
        if not match:
            logger.warning(f"ç—…ä¾‹ {case_id}{section_info} æœªæ‰¾åˆ°JSONæ•°ç»„æ ¼å¼")
            return []
        
        json_str = match.group(0)
        
        # 2. æ£€æŸ¥JSONæ˜¯å¦å®Œæ•´
        try:
            extractions = json.loads(json_str)
            if not isinstance(extractions, list): 
                logger.warning(f"ç—…ä¾‹ {case_id}{section_info} JSONè§£æç»“æœä¸æ˜¯æ•°ç»„")
                return []
            logger.success(f"ç—…ä¾‹ {case_id}{section_info} JSONè§£ææˆåŠŸï¼Œæå– {len(extractions)} æ¡è®°å½•")
        except json.JSONDecodeError as e:
            logger.warning(f"ç—…ä¾‹ {case_id}{section_info} JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            
            # 3. å°è¯•ä¿®å¤è¢«æˆªæ–­çš„JSON
            objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', json_str)
            if objects:
                fixed_json = "[" + ",".join(objects) + "]"
                try:
                    extractions = json.loads(fixed_json)
                    if not isinstance(extractions, list):
                        logger.warning(f"ç—…ä¾‹ {case_id}{section_info} ä¿®å¤åJSONä»ä¸æ˜¯æ•°ç»„")
                        return []
                    logger.info(f"ç—…ä¾‹ {case_id}{section_info} JSONä¿®å¤æˆåŠŸï¼Œä½¿ç”¨ {len(extractions)} ä¸ªå®Œæ•´å¯¹è±¡")
                except json.JSONDecodeError:
                    logger.error(f"ç—…ä¾‹ {case_id}{section_info} JSONä¿®å¤å¤±è´¥")
                    return []
            else:
                logger.error(f"ç—…ä¾‹ {case_id}{section_info} æ— æ³•æ‰¾åˆ°ä»»ä½•å®Œæ•´JSONå¯¹è±¡")
                return []
                
    except Exception as e:
        logger.error(f"ç—…ä¾‹ {case_id}{section_info} å¤„ç†LLMå“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

    valid_extractions = []
    for ext in extractions:
        ext['case_id'] = case_id
        # æ·»åŠ ç« èŠ‚ä¿¡æ¯ç”¨äºè¿½æº¯
        if section_name:
            ext['source_section'] = section_name
        if validate_extraction(ext):
            valid_extractions.append(ext)
    return valid_extractions

def main():
    parser = argparse.ArgumentParser(description="å¹¶è¡Œè’¸é¦å·¥ä½œè„šæœ¬")
    parser.add_argument("--input_dir", type=str, help="åŒ…å«åŸå§‹.txtæŠ¥å‘Šçš„ç›®å½• (ä¸-start_index, -end_indexé…åˆä½¿ç”¨)")
    parser.add_argument("--output_dir", type=str, required=True, help="å­˜æ”¾ç‹¬ç«‹è¾“å‡ºJSONæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--api_key_name", type=str, required=True, help="åœ¨system_config.pyä¸­å®šä¹‰çš„APIé…ç½®å (e.g., api_1)")
    parser.add_argument("--start_index", type=int, help="å¤„ç†çš„èµ·å§‹æ–‡ä»¶ç´¢å¼•")
    parser.add_argument("--end_index", type=int, help="å¤„ç†çš„ç»“æŸæ–‡ä»¶ç´¢å¼•")
    parser.add_argument("--file_list", type=str, help="ä¸€ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«éœ€è¦å¤„ç†çš„æŠ¥å‘Šæ–‡ä»¶çš„ç»å¯¹è·¯å¾„åˆ—è¡¨ (ä¼˜å…ˆäº-input_dir)")
    parser.add_argument("--prompt_type", type=str, default="universal", choices=["universal", "restricted"], 
                       help="æç¤ºè¯ç±»å‹: universal(æ‰€æœ‰å™¨å®˜) æˆ– restricted(ä»…5ä¸ªæŒ‡å®šå™¨å®˜)")
    args = parser.parse_args()

    # æ ¡éªŒå‚æ•°
    if args.file_list:
        logger.info(f"ğŸš€ Workerä»¥æ–‡ä»¶åˆ—è¡¨æ¨¡å¼å¯åŠ¨: API='{args.api_key_name}', ä»»åŠ¡æ–‡ä»¶='{args.file_list}'...")
    elif args.input_dir and args.start_index is not None and args.end_index is not None:
        logger.info(f"ğŸš€ Workerä»¥èŒƒå›´æ¨¡å¼å¯åŠ¨: API='{args.api_key_name}', èŒƒå›´=[{args.start_index}, {args.end_index})...")
    else:
        logger.error("å‚æ•°é”™è¯¯: å¿…é¡»æä¾› '--file_list' æˆ–è€… '--input_dir', '--start_index' å’Œ '--end_index' ä¸‰è€…ã€‚")
        return

    # 1. åˆå§‹åŒ–å·¥å…·ã€æç¤ºå’Œä¸ºè¯¥workeråˆ›å»ºä¸“å±è¾“å‡ºå­ç›®å½•
    api_config = MULTI_API_CONFIG.get(args.api_key_name)
    if not api_config:
        logger.error(f"æœªåœ¨system_config.pyä¸­æ‰¾åˆ°åä¸º '{args.api_key_name}' çš„APIé…ç½®ã€‚")
        return
        
    llm_extractor = LLMExtractor(
        model=api_config["model"],
        api_key=api_config["api_key"],
        base_url=api_config["base_url"]
    )
    # æ ¹æ®å‚æ•°é€‰æ‹©æç¤ºè¯ç±»å‹
    if args.prompt_type == "universal":
        inference_prompt = MedicalExtractionPrompts.get_universal_inference_prompt()
        logger.info("ä½¿ç”¨é€šç”¨æ¨ç†æç¤ºè¯ - æ”¯æŒæ‰€æœ‰å™¨å®˜ç³»ç»Ÿ")
    else:
        inference_prompt = MedicalExtractionPrompts.get_inference_prompt()
        logger.info("ä½¿ç”¨é™åˆ¶æ€§æ¨ç†æç¤ºè¯ - ä»…æ”¯æŒ5ä¸ªæŒ‡å®šå™¨å®˜")
    
    # ä¸ºæ¯ä¸ªworkeråˆ›å»ºä¸€ä¸ªä¸“å±çš„å­ç›®å½•
    worker_output_dir = os.path.join(args.output_dir, f"worker_{args.api_key_name}")
    os.makedirs(worker_output_dir, exist_ok=True)
    
    # åˆ›å»ºè¯¦ç»†è¾“å‡ºç›®å½•ç»“æ„
    detailed_dirs = {
        "json": os.path.join(worker_output_dir, "json_results"),
        "logs": os.path.join(worker_output_dir, "processing_logs"), 
        "thinking": os.path.join(worker_output_dir, "thinking_chains")
    }
    
    for dir_path in detailed_dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    logger.info(f"Workerçš„è¾“å‡ºå°†è¢«ä¿å­˜åˆ°: {worker_output_dir}")
    logger.info(f"è¯¦ç»†ç»“æœç›®å½•: JSON={detailed_dirs['json']}, LOG={detailed_dirs['logs']}, TXT={detailed_dirs['thinking']}")

    # 2. æ ¹æ®æ¨¡å¼åŠ è½½æ•°æ®
    if args.file_list:
        reports_to_process = load_reports_from_list(args.file_list)
    else:
        reports_to_process = load_reports_in_range(args.input_dir, args.start_index, args.end_index)

    if not reports_to_process:
        logger.warning("æœªåŠ è½½åˆ°ä»»ä½•æŠ¥å‘Šï¼ŒWorkeré€€å‡ºã€‚")
        return

    # 3. å¾ªç¯å¤„ç†ï¼Œå¹¶å°†æ¯ä¸ªæŠ¥å‘Šçš„ç»“æœä¿å­˜ä¸ºç‹¬ç«‹æ–‡ä»¶
    total_processed_count = 0
    for report in reports_to_process:
        case_id = report.get('case_id', 'unknown')
        original_filename_base = os.path.splitext(report.get('filename', f"report_{case_id}"))[0]
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶ï¼Œè®°å½•è¯¥æŠ¥å‘Šçš„å¤„ç†è¿‡ç¨‹
        processing_log_file = os.path.join(detailed_dirs['logs'], f"{original_filename_base}_processing.log")
        
        # åˆ›å»ºæŠ¥å‘Šä¸“ç”¨çš„logger
        report_logger = logger.bind(report_id=case_id, log_file=processing_log_file)
        report_logger.info(f"å¼€å§‹å¤„ç†æŠ¥å‘Š {case_id} ({report.get('filename')})")
        
        # å¤„ç†æŠ¥å‘Šå¹¶æ”¶é›†æ€è€ƒé“¾
        thinking_chain = []
        valid_extractions = process_report_with_thinking(
            report, llm_extractor, inference_prompt, thinking_chain, report_logger
        )
        
        # ä¿å­˜JSONç»“æœ
        if valid_extractions:
            json_result_file = os.path.join(detailed_dirs['json'], f"{original_filename_base}_extracted.json")
            try:
                result_data = {
                    "report_info": {
                        "case_id": case_id,
                        "filename": report.get('filename'),
                        "processing_time": str(datetime.now()),
                        "api_used": args.api_key_name,
                        "total_extractions": len(valid_extractions)
                    },
                    "extractions": valid_extractions
                }
                
                with open(json_result_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                total_processed_count += len(valid_extractions)
                report_logger.success(f"JSONç»“æœå·²ä¿å­˜: {json_result_file}")
                
            except Exception as e:
                report_logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        
        # ä¿å­˜æ€è€ƒé“¾
        thinking_file = os.path.join(detailed_dirs['thinking'], f"{original_filename_base}_thinking.txt")
        try:
            with open(thinking_file, 'w', encoding='utf-8') as f:
                f.write(f"æŠ¥å‘Š {case_id} çš„å¤„ç†æ€è€ƒé“¾\n")
                f.write("=" * 50 + "\n\n")
                for i, step in enumerate(thinking_chain, 1):
                    f.write(f"æ­¥éª¤ {i}: {step['action']}\n")
                    f.write(f"æ—¶é—´: {step['timestamp']}\n") 
                    f.write(f"è¯¦æƒ…: {step['details']}\n")
                    f.write("-" * 30 + "\n\n")
            
            report_logger.success(f"æ€è€ƒé“¾å·²ä¿å­˜: {thinking_file}")
            
        except Exception as e:
            report_logger.error(f"ä¿å­˜æ€è€ƒé“¾æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆ›å»ºå¤„ç†æ—¥å¿—çš„æ‘˜è¦
        log_summary_file = os.path.join(detailed_dirs['logs'], f"{original_filename_base}_summary.log")
        try:
            with open(log_summary_file, 'w', encoding='utf-8') as f:
                f.write(f"æŠ¥å‘Šå¤„ç†æ‘˜è¦ - {case_id}\n")
                f.write("=" * 40 + "\n")
                f.write(f"æ–‡ä»¶å: {report.get('filename')}\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now()}\n")
                f.write(f"ä½¿ç”¨API: {args.api_key_name}\n")
                f.write(f"æå–è®°å½•æ•°: {len(valid_extractions)}\n")
                f.write(f"å¤„ç†çŠ¶æ€: {'æˆåŠŸ' if valid_extractions else 'å¤±è´¥'}\n")
                f.write("-" * 40 + "\n")
                
                if valid_extractions:
                    f.write("æå–çš„ç—‡çŠ¶æ¦‚è¦:\n")
                    for i, ext in enumerate(valid_extractions[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
                        f.write(f"{i}. {ext.get('symptom_or_disease', 'N/A')} -> {ext.get('inferred_organ', 'N/A')}\n")
                    if len(valid_extractions) > 10:
                        f.write(f"... è¿˜æœ‰ {len(valid_extractions) - 10} æ¡è®°å½•\n")
                
        except Exception as e:
            report_logger.error(f"ä¿å­˜æ‘˜è¦æ—¥å¿—å¤±è´¥: {e}")

    logger.success(f"âœ… Workerä»»åŠ¡å®Œæˆã€‚æ€»å…±ç”Ÿæˆ {total_processed_count} æ¡é«˜è´¨é‡è¯­æ–™ã€‚")
    logger.info(f"è¯¦ç»†ç»“æœå·²åˆ†ç±»ä¿å­˜åœ¨:")
    logger.info(f"  JSONç»“æœ: {detailed_dirs['json']}/")
    logger.info(f"  å¤„ç†æ—¥å¿—: {detailed_dirs['logs']}/") 
    logger.info(f"  æ€è€ƒé“¾: {detailed_dirs['thinking']}/")

def process_report_with_thinking(report: Dict[str, Any], extractor: LLMExtractor, 
                               prompt: str, thinking_chain: List[Dict], report_logger) -> List[Dict[str, Any]]:
    """å¸¦æ€è€ƒé“¾è®°å½•çš„æŠ¥å‘Šå¤„ç†å‡½æ•°"""
    from datetime import datetime
    
    text, case_id = report.get("text", ""), report.get("case_id", "N/A")
    
    thinking_chain.append({
        "action": "å¼€å§‹å¤„ç†æŠ¥å‘Š",
        "timestamp": str(datetime.now()),
        "details": f"æŠ¥å‘Šé•¿åº¦: {len(text)} å­—ç¬¦"
    })
    
    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™é‡‡ç”¨ç»“æ„åŒ–åˆ†å—
    if len(text) > 4000:
        thinking_chain.append({
            "action": "æ–‡æœ¬åˆ†å—å†³ç­–",
            "timestamp": str(datetime.now()),
            "details": f"æ–‡æœ¬è¶…è¿‡4000å­—ç¬¦ï¼Œé‡‡ç”¨ç»“æ„åŒ–åˆ†å—å¤„ç†"
        })
        
        text_chunks = smart_chunk_medical_report(text)
        
        if not text_chunks:
            thinking_chain.append({
                "action": "åˆ†å—å¤±è´¥",
                "timestamp": str(datetime.now()),
                "details": "æœªèƒ½è¯†åˆ«åˆ°æœ‰æ•ˆç« èŠ‚ï¼Œè·³è¿‡å¤„ç†"
            })
            return []
        
        thinking_chain.append({
            "action": "åˆ†å—æˆåŠŸ",
            "timestamp": str(datetime.now()),
            "details": f"åˆ†å‰²ä¸º {len(text_chunks)} ä¸ªç« èŠ‚å—"
        })
        
        all_extractions = []
        successful_chunks = 0
        
        for i, chunk_info in enumerate(text_chunks):
            section_name = chunk_info["section"]
            chunk_content = chunk_info["content"]
            
            thinking_chain.append({
                "action": f"å¤„ç†ç« èŠ‚ {i+1}/{len(text_chunks)}",
                "timestamp": str(datetime.now()),
                "details": f"ç« èŠ‚: {section_name}, é•¿åº¦: {len(chunk_content)} å­—ç¬¦"
            })
            
            enhanced_prompt = prompt + f"\n\n--- Medical Report Section: {section_name} ---\n" + chunk_content
            result = extractor.call_api(enhanced_prompt)
            
            if not (result and result.get("success")):
                thinking_chain.append({
                    "action": "APIè°ƒç”¨å¤±è´¥",
                    "timestamp": str(datetime.now()),
                    "details": f"ç« èŠ‚ '{section_name}' çš„LLMè°ƒç”¨å¤±è´¥"
                })
                continue
            
            chunk_extractions = parse_llm_response(result.get("response", "[]"), case_id, section_name)
            if chunk_extractions:
                all_extractions.extend(chunk_extractions)
                successful_chunks += 1
                
                thinking_chain.append({
                    "action": "ç« èŠ‚å¤„ç†æˆåŠŸ",
                    "timestamp": str(datetime.now()),
                    "details": f"ä»ç« èŠ‚ '{section_name}' æå–äº† {len(chunk_extractions)} æ¡è®°å½•"
                })
        
        thinking_chain.append({
            "action": "åˆ†å—å¤„ç†å®Œæˆ",
            "timestamp": str(datetime.now()),
            "details": f"æˆåŠŸå¤„ç† {successful_chunks}/{len(text_chunks)} ä¸ªç« èŠ‚ï¼Œæ€»æå– {len(all_extractions)} æ¡è®°å½•"
        })
        
        return all_extractions
    else:
        thinking_chain.append({
            "action": "å•æ¬¡å¤„ç†å†³ç­–",
            "timestamp": str(datetime.now()),
            "details": f"æ–‡æœ¬è¾ƒçŸ­({len(text)}å­—ç¬¦)ï¼Œé‡‡ç”¨å•æ¬¡å¤„ç†"
        })
        
        full_prompt = prompt + "\n\n--- Medical Report ---\n" + text
        result = extractor.call_api(full_prompt)

        if not (result and result.get("success")):
            thinking_chain.append({
                "action": "APIè°ƒç”¨å¤±è´¥",
                "timestamp": str(datetime.now()),
                "details": "LLMè°ƒç”¨å¤±è´¥"
            })
            return []
        
        extractions = parse_llm_response(result.get("response", "[]"), case_id)
        
        thinking_chain.append({
            "action": "å•æ¬¡å¤„ç†å®Œæˆ",
            "timestamp": str(datetime.now()),
            "details": f"æå– {len(extractions)} æ¡è®°å½•"
        })
        
        return extractions


if __name__ == "__main__":
    main() 