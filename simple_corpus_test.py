#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„è¯­æ–™åˆ†ææµ‹è¯•è„šæœ¬
"""

import json
import sys
import os

sys.path.append('/opt/RAG_Evidence4Organ')

from Question_Distillation_v2.extractors.llm_extractor import LLMExtractor
from configs.system_config import MULTI_API_CONFIG

def test_api_call():
    """æµ‹è¯•APIè°ƒç”¨æ˜¯å¦æ­£å¸¸"""
    
    # ä½¿ç”¨api_16
    api_config = MULTI_API_CONFIG['api_16']
    extractor = LLMExtractor(
        model=api_config['model'],
        api_key=api_config['api_key'],
        base_url=api_config['base_url']
    )
    
    # ç®€å•çš„æµ‹è¯•æç¤ºè¯
    simple_prompt = """
è¯·åˆ†æä»¥ä¸‹åŒ»ç–—æ–‡æœ¬ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

{
  "document_type": "æŠ¥å‘Šç±»å‹",
  "has_symptoms": true/false,
  "has_diagnosis": true/false,
  "brief_summary": "ç®€è¦æ€»ç»“"
}

åŒ»ç–—æ–‡æœ¬ï¼š
58f w/rheumatoid arthritis on periodic prednisone, htn who presented to osh on w/sob, f/c, productive cough x 1 wk.

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
"""
    
    try:
        print("å¼€å§‹æµ‹è¯•APIè°ƒç”¨...")
        response = extractor.call_api(simple_prompt)
        print("APIå“åº”æˆåŠŸ:")
        print(response)
        print("\n" + "="*50)
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(response)
            print("JSONè§£ææˆåŠŸ:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print("åŸå§‹å“åº”:")
            print(repr(response))
            
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {e}")

def analyze_sample_text():
    """åˆ†ææ ·æœ¬æ–‡æœ¬ç»“æ„"""
    
    # è¯»å–æ ·æœ¬æ–‡ä»¶
    with open('dataset/report_6001.txt', 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    print("æ ·æœ¬æ–‡æœ¬å†…å®¹ (å‰500å­—ç¬¦):")
    print(content[:500])
    print("\n" + "="*50)
    
    print(f"æ–‡æœ¬é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"æ–‡æœ¬è¡Œæ•°: {len(content.splitlines())}")
    
    # åˆ†ææ–‡æœ¬ç»“æ„
    lines = content.split('\n')
    print(f"\næ–‡æœ¬ç»“æ„åˆ†æ:")
    print(f"æ€»è¡Œæ•°: {len(lines)}")
    
    for i, line in enumerate(lines[:10]):  # åªæ˜¾ç¤ºå‰10è¡Œ
        print(f"è¡Œ {i+1}: {line[:100]}...")
        
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§åŒ»ç–—å…³é”®è¯
    keywords = {
        'symptoms': ['pain', 'cough', 'fever', 'nausea', 'shortness of breath', 'sob', 'fatigue'],
        'diagnoses': ['diagnosis', 'pneumonia', 'arthritis', 'hypertension', 'htn'],
        'treatments': ['treatment', 'medication', 'surgery', 'therapy'],
        'procedures': ['procedure', 'catheterization', 'intubation']
    }
    
    print(f"\nå…³é”®è¯åˆ†æ:")
    for category, words in keywords.items():
        found_words = [word for word in words if word.lower() in content.lower()]
        print(f"{category}: {found_words}")

if __name__ == "__main__":
    print("ğŸ” è¯­æ–™ç»“æ„è¯Šæ–­æµ‹è¯•")
    print("="*50)
    
    print("\n1. åˆ†ææ ·æœ¬æ–‡æœ¬ç»“æ„:")
    analyze_sample_text()
    
    print("\n2. æµ‹è¯•APIè°ƒç”¨:")
    test_api_call() 