#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å»ºRAGè¯­æ–™åº“è„šæœ¬ (V3 - æ¨ç†å‹è¯­æ–™åŠ è½½å™¨)
å°†ç”± generate_corpus.py ç”Ÿæˆçš„æ¨ç†å‹è¯­æ–™åŠ è½½åˆ°å‘é‡æ•°æ®åº“
"""

import sys
import os
import json
import argparse
from typing import List, Dict, Any
from loguru import logger

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from rag_system.storage.chroma_storage import ChromaStorage
from rag_system.models.bio_lm_embedding import BioLMEmbedding
from rag_system.models.embedding_function_adapter import ChromaEmbeddingFunctionAdapter
from configs.system_config import RAG_CONFIG

def load_inferred_corpus(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½æ¨ç†å‹è¯­æ–™åº“JSONæ–‡ä»¶"""
    logger.info(f"æ­£åœ¨åŠ è½½æ¨ç†å‹è¯­æ–™æ–‡ä»¶: {file_path}...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            logger.success(f"æˆåŠŸåŠ è½½ {len(data)} æ¡æ¨ç†è¯­æ–™ã€‚")
            return data
        else:
            logger.error(f"æ–‡ä»¶ {file_path} å†…å®¹ä¸æ˜¯ä¸€ä¸ªJSONåˆ—è¡¨ã€‚")
            return []
    except FileNotFoundError:
        logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def prepare_documents_from_inferred(corpus: List[Dict[str, Any]]) -> (List[str], List[Dict[str, Any]], List[str]):
    """
    ä»æ¨ç†å‹è¯­æ–™å‡†å¤‡ç”¨äºç´¢å¼•çš„æ–‡æ¡£ã€å…ƒæ•°æ®å’ŒIDã€‚
    æ”¯æŒé€šç”¨å™¨å®˜æå–ç»“æœã€‚
    """
    logger.info("æ­£åœ¨ä»æ¨ç†å‹è¯­æ–™å‡†å¤‡ç´¢å¼•æ–‡æ¡£...")
    
    documents = []
    metadatas = []
    ids = []
    
    organ_stats = {'specified': 0, 'other': 0}
    
    for i, item in enumerate(corpus):
        symptom = item.get('symptom_or_disease', '')
        parts = item.get('suggested_anatomical_parts_to_examine', [])
        organ = item.get('inferred_organ', '')
        organ_category = item.get('organ_category', 'unknown')
        
        # å°†æ ¸å¿ƒæ¨ç†ä¿¡æ¯ç»„åˆæˆå¾…å‘é‡åŒ–çš„æ–‡æ¡£
        parts_string = ", ".join(parts)
        content_to_vectorize = f"Symptom: {symptom}. Organ: {organ}. Suggested examination for: {parts_string}."
        
        if not symptom or not parts or not organ:
            logger.warning(f"è·³è¿‡ç´¢å¼• {i}ï¼Œå› ä¸ºç—‡çŠ¶ã€å™¨å®˜æˆ–å»ºè®®éƒ¨ä½ä¸ºç©ºã€‚")
            continue
        
        # æ‰€æœ‰åŸå§‹ä¿¡æ¯éƒ½æˆä¸ºå…ƒæ•°æ®
        metadata = {str(k): str(v) for k, v in item.items()}
        
        # åˆ›å»ºå”¯ä¸€ID
        unique_id = f"{item.get('case_id', 'caseN_A')}_symptom_{i}"
        
        documents.append(content_to_vectorize)
        metadatas.append(metadata)
        ids.append(unique_id)
        
        # ç»Ÿè®¡å™¨å®˜ç±»åˆ«
        if organ_category in organ_stats:
            organ_stats[organ_category] += 1
        
    logger.success(f"æˆåŠŸå‡†å¤‡ {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºç´¢å¼•ã€‚")
    logger.info(f"å™¨å®˜ç±»åˆ«ç»Ÿè®¡: æŒ‡å®šå™¨å®˜ {organ_stats['specified']} æ¡, å…¶ä»–å™¨å®˜ {organ_stats['other']} æ¡")
    
    return documents, metadatas, ids

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä»æ¨ç†å‹è¯­æ–™æ„å»ºRAGç³»ç»Ÿçš„å‘é‡æ•°æ®åº“")
    parser.add_argument(
        'corpus_file', 
        type=str,
        help="ç”±generate_corpus.pyç”Ÿæˆçš„æ¨ç†å‹JSONè¯­æ–™æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        '--collection_name', 
        type=str, 
        default=RAG_CONFIG.get("collection_name_inferred", "medical_evidence_inferred"),
        help="åœ¨å‘é‡æ•°æ®åº“ä¸­åˆ›å»ºçš„é›†åˆï¼ˆCollectionï¼‰åç§°"
    )
    parser.add_argument(
        '--chunk_size', 
        type=int, 
        default=500,
        help="æ‰¹é‡å¤„ç†çš„æ–‡æ¡£æ•°é‡"
    )
    args = parser.parse_args()

    logger.info("ğŸš€ å¼€å§‹æ„å»ºRAGå‘é‡è¯­æ–™åº“ (ä»æ¨ç†å‹è¯­æ–™)...")
    
    # 1. åŠ è½½è¯­æ–™æ–‡ä»¶
    corpus_data = load_inferred_corpus(args.corpus_file)
    if not corpus_data:
        logger.error("æœªèƒ½åŠ è½½ä»»ä½•è¯­æ–™æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return
        
    # 2. å‡†å¤‡æ–‡æ¡£
    documents, metadatas, ids = prepare_documents_from_inferred(corpus_data)
    
    # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    logger.info("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (BioLMEmbedding)...")
    embedding_model = BioLMEmbedding()
    
    # 4. åˆ›å»ºåµŒå…¥å‡½æ•°é€‚é…å™¨
    logger.info("åˆ›å»ºChromaDBåµŒå…¥å‡½æ•°é€‚é…å™¨...")
    chroma_embedding_function = ChromaEmbeddingFunctionAdapter(embedding_model)
    
    # 5. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    logger.info(f"æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼Œé›†åˆåç§°: '{args.collection_name}'...")
    vector_store = ChromaStorage(
        collection_name=args.collection_name,
        embedding_function=chroma_embedding_function
    )
    
    # 6. æ¸…ç©ºæ—§æ•°æ®
    logger.info("æ­£åœ¨æ¸…ç©ºæ—§çš„é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰...")
    vector_store.reset_collection()
    
    # 7. åˆ†æ‰¹æ¬¡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    logger.info(f"å¼€å§‹å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£...")
    
    total_docs = len(documents)
    for i in range(0, total_docs, args.chunk_size):
        batch_docs = documents[i:i + args.chunk_size]
        batch_metadatas = metadatas[i:i + args.chunk_size]
        batch_ids = ids[i:i + args.chunk_size]
        
        logger.info(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡: {i // args.chunk_size + 1} / {total_docs // args.chunk_size + 1}...")
        
        vector_store.add_documents(
            documents=batch_docs,
            metadatas=batch_metadatas,
            ids=batch_ids
        )
    
    logger.success("ğŸ‰ğŸ‰ğŸ‰ RAGå‘é‡è¯­æ–™åº“æ„å»ºå®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰")
    logger.info(f"æ€»è®¡ {vector_store.count()} ä¸ªæ–‡æ¡£å·²æˆåŠŸå­˜å…¥é›†åˆ '{args.collection_name}'ã€‚")

if __name__ == "__main__":
    main() 