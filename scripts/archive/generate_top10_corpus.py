#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ä»æŠ¥å‘Šæ–‡ä»¶ä¸­æå–æœ€é‡è¦çš„10ä¸ªå¥å­ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªè¯­æ–™åº“JSONæ–‡ä»¶ã€‚
"""
import sys
import os
import json
import time
from pathlib import Path
from loguru import logger
from tqdm import tqdm

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# æ­£å¸¸å¯¼å…¥é¡¹ç›®æ¨¡å—
from RAG_Evidence4Organ.configs.system_config import MULTI_API_CONFIG
from RAG_Evidence4Organ.knowledge_distillation.extractors.llm_extractor import LLMExtractor

def get_extraction_prompt(report_text: str) -> str:
    """ç”Ÿæˆç”¨äºæå–çš„ç²¾ç¡®æç¤ºè¯"""
    return f"""
    Please analyze the following medical report. Your task is to act as a medical expert and extract the 10 most important and descriptive sentences that describe diseases, symptoms, or abnormal findings.

    Guidelines:
    - Focus on sentences that provide clear, specific information about the patient's condition.
    - Prioritize primary diagnoses, acute problems, and significant test results.
    - Do NOT extract medication lists, social history, or normal findings unless they are directly explaining a pathology.
    - The output MUST be a JSON object with a single key "top_10_sentences", and its value must be a list of exactly 10 strings.
    - Each string in the list must be a direct quote from the report. Do not summarize or create new sentences.

    Medical Report:
    ---
    {report_text}
    ---

    Now, please extract the top 10 sentences and provide them in the specified JSON format.
    """

def parse_llm_response(response_content: str) -> list | None:
    """å°è¯•è§£æLLMè¿”å›çš„JSONå“åº”"""
    try:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' æ¥æå–JSONå—
        json_start = response_content.find('{')
        json_end = response_content.rfind('}') + 1
        if json_start != -1 and json_end != -1:
            json_str = response_content[json_start:json_end]
            parsed_json = json.loads(json_str)
            sentences = parsed_json.get("top_10_sentences")
            if isinstance(sentences, list):
                return sentences
        logger.warning(f"æ— æ³•ä»å“åº”ä¸­æ­£ç¡®è§£æå‡º 'top_10_sentences' åˆ—è¡¨: {response_content}")
        return None
    except json.JSONDecodeError:
        logger.warning(f"æœªèƒ½å°†å“åº”ä½œä¸ºJSONè§£æ: {response_content}")
        return None


def main(start_id: int, end_id: int):
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ‰¹é‡æå–ä»»åŠ¡ã€‚
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æå–Top 10å¥å­ï¼ŒèŒƒå›´: {start_id}-{end_id}")

    # 1. åˆå§‹åŒ–æå–å™¨
    try:
        api_config = MULTI_API_CONFIG.get("api_1")
        if not api_config:
            logger.error("âŒ åœ¨ system_config.py ä¸­æœªæ‰¾åˆ° 'api_1' é…ç½®ã€‚")
            return
        extractor = LLMExtractor(
            model=api_config["model"],
            api_key=api_config["api_key"],
            base_url=api_config["base_url"]
        )
        logger.success("âœ… LLMæå–å™¨åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–LLMæå–å™¨å¤±è´¥: {e}")
        return

    # 2. å‡†å¤‡è·¯å¾„å’Œæ•°æ®ç»“æ„
    dataset_dir = PROJECT_ROOT / "RAG_Evidence4Organ" / "dataset"
    output_file = PROJECT_ROOT / f"top10_sentences_corpus_{start_id}-{end_id}.json"
    all_results = []
    
    # 3. å¾ªç¯å¤„ç†æ–‡ä»¶
    report_ids = range(start_id, end_id + 1)
    for report_id in tqdm(report_ids, desc="å¤„ç†æŠ¥å‘Š"):
        report_file_path = dataset_dir / f"report_{report_id}.txt"
        
        if not report_file_path.exists():
            logger.warning(f"âš ï¸ æŠ¥å‘Š {report_id}: æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå·²è·³è¿‡ã€‚")
            continue

        try:
            report_text = report_file_path.read_text(encoding="utf-8")
            prompt = get_extraction_prompt(report_text)
            
            # è°ƒç”¨API
            result = extractor.call_api(prompt)

            if result["success"]:
                sentences = parse_llm_response(result["response"])
                if sentences:
                    all_results.append({
                        "case_id": str(report_id),
                        "sentences": sentences
                    })
                    logger.success(f"âœ… æŠ¥å‘Š {report_id}: æˆåŠŸæå– {len(sentences)} æ¡å¥å­ã€‚")
                else:
                    logger.error(f"âŒ æŠ¥å‘Š {report_id}: æˆåŠŸè°ƒç”¨APIä½†æ— æ³•è§£æå‡ºæœ‰æ•ˆå¥å­ã€‚")
            else:
                logger.error(f"âŒ æŠ¥å‘Š {report_id}: LLM APIè°ƒç”¨å¤±è´¥: {result['error']}")
        
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Š {report_id}: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        
        # å¢åŠ ä¸€ç‚¹å»¶è¿Ÿé¿å…APIè¿‡è½½
        time.sleep(1)

    # 4. ä¿å­˜ç»“æœ
    logger.info(f"ğŸ’¾ æ­£åœ¨å°† {len(all_results)} æ¡ç»“æœä¿å­˜åˆ°æ–‡ä»¶: {output_file}")
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.success(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼è¯­æ–™åº“å·²æˆåŠŸä¿å­˜ã€‚")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="æ‰¹é‡æå–åŒ»ç–—æŠ¥å‘Šä¸­çš„Top 10å…³é”®å¥ã€‚")
    parser.add_argument("--start", type=int, default=40000, help="èµ·å§‹æŠ¥å‘ŠID")
    parser.add_argument("--end", type=int, default=41000, help="ç»“æŸæŠ¥å‘ŠID")
    args = parser.parse_args()
    
    main(args.start, args.end) 