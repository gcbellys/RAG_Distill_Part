#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»“æœèšåˆè„šæœ¬
ä½œè€…: Gemini & CDJ_LP
æè¿°:
è¯¥è„šæœ¬ç”¨äºå°†åœ¨å¹¶è¡Œå¤„ç†ä¸­ç”Ÿæˆçš„å¤šä¸ªç‹¬ç«‹JSONç»“æœæ–‡ä»¶ï¼Œ
åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„ã€æœ€ç»ˆçš„è¯­æ–™åº“æ–‡ä»¶ã€‚

æ‰§è¡Œæ–¹æ³•:
python RAG_Evidence4Organ/scripts/aggregate_results.py \
    --input_dir RAG_Evidence4Organ/knowledge_distillation/results/parallel_outputs/ \
    --output_file RAG_Evidence4Organ/data/corpus_inferred.json
"""

import os
import json
import argparse
from typing import List, Dict, Any
from loguru import logger

def aggregate_json_files(input_dir: str, output_file: str):
    """
    éå†ç›®å½•ä¸­çš„æ‰€æœ‰workerå­ç›®å½•ï¼Œå¹¶å°†å®ƒä»¬åŒ…å«çš„ç‹¬ç«‹JSONæ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„è¯­æ–™åº“ã€‚
    æ”¯æŒé€šç”¨å™¨å®˜æå–ç»“æœã€‚
    """
    logger.info(f"ğŸš€ å¼€å§‹ä»çˆ¶ç›®å½• '{input_dir}' èšåˆç»“æœ...")
    
    if not os.path.isdir(input_dir):
        logger.error(f"é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨ -> {input_dir}")
        return

    all_data: List[Dict[str, Any]] = []
    organ_stats = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰worker_*å­ç›®å½•
    try:
        worker_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d)) and d.startswith('worker_')]
    except FileNotFoundError:
        logger.error(f"æ— æ³•è®¿é—®ç›®å½•: {input_dir}ã€‚è¯·æ£€æŸ¥è·¯å¾„å’Œæƒé™ã€‚")
        return
        
    if not worker_dirs:
        logger.warning(f"åœ¨ç›®å½• '{input_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½• 'worker_*' å­ç›®å½•è¿›è¡Œèšåˆã€‚")
        return

    logger.info(f"æ‰¾åˆ°äº† {len(worker_dirs)} ä¸ªworkerå­ç›®å½•è¿›è¡Œå¤„ç†ã€‚")

    for worker_dir in worker_dirs:
        worker_path = os.path.join(input_dir, worker_dir)
        logger.info(f"æ­£åœ¨å¤„ç†å­ç›®å½•: {worker_dir}...")
        
        result_files = [f for f in os.listdir(worker_path) if f.endswith('_processed.json')]
        
        for filename in result_files:
            file_path = os.path.join(worker_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_data.extend(data)
                        # ç»Ÿè®¡å™¨å®˜åˆ†å¸ƒ
                        for item in data:
                            organ = item.get('inferred_organ', 'Unknown')
                            organ_category = item.get('organ_category', 'unknown')
                            if organ not in organ_stats:
                                organ_stats[organ] = {'count': 0, 'category': organ_category}
                            organ_stats[organ]['count'] += 1
                    else:
                        logger.warning(f"æ–‡ä»¶ {filename} çš„å†…å®¹ä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå·²è·³è¿‡ã€‚")
            except json.JSONDecodeError:
                logger.error(f"è§£æJSONå¤±è´¥: {filename}ï¼Œå·²è·³è¿‡ã€‚")
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}ï¼Œå·²è·³è¿‡ã€‚")

    logger.success(f"èšåˆå®Œæˆï¼Œæ€»å…±åˆå¹¶äº† {len(all_data)} æ¡è®°å½•ã€‚")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š å™¨å®˜åˆ†å¸ƒç»Ÿè®¡:")
    specified_organs = []
    other_organs = []
    
    for organ, stats in sorted(organ_stats.items(), key=lambda x: x[1]['count'], reverse=True):
        category = stats['category']
        count = stats['count']
        if category == 'specified':
            specified_organs.append(f"{organ}: {count}")
        else:
            other_organs.append(f"{organ}: {count}")
    
    if specified_organs:
        logger.info("  ğŸ“ æŒ‡å®šå™¨å®˜ (5ä¸ªæ ¸å¿ƒå™¨å®˜):")
        for stat in specified_organs:
            logger.info(f"    {stat}")
    
    if other_organs:
        logger.info("  ğŸŒ å…¶ä»–å™¨å®˜:")
        for stat in other_organs:
            logger.info(f"    {stat}")

    try:
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶çš„ç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        logger.success(f"âœ… æœ€ç»ˆè¯­æ–™åº“å·²æˆåŠŸä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶å¤šä¸ªJSONç»“æœæ–‡ä»¶")
    parser.add_argument("--input_dir", type=str, required=True, help="åŒ…å«ç‹¬ç«‹JSONæ–‡ä»¶çš„è¾“å…¥ç›®å½•")
    parser.add_argument("--output_file", type=str, required=True, help="æœ€ç»ˆåˆå¹¶åçš„JSONæ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()
    
    aggregate_json_files(args.input_dir, args.output_file)

if __name__ == "__main__":
    main() 