#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç»­ä¼ ä»»åŠ¡ç”Ÿæˆè„šæœ¬
ä½œè€…: Gemini & CDJ_LP
æè¿°:
è¯¥è„šæœ¬ç”¨äºå®ç°æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ã€‚å®ƒä¼šå¯¹æ¯”ç›®æ ‡æ–‡ä»¶å’Œå·²å®Œæˆæ–‡ä»¶ï¼Œ
æ‰¾å‡ºæ‰€æœ‰æœªå¤„ç†çš„æŠ¥å‘Šï¼Œç„¶åå°†è¿™äº›ä»»åŠ¡å¹³å‡åˆ†é…ç»™å¤šä¸ªAPIï¼Œ
ä¸ºæ¯ä¸ªAPIç”Ÿæˆä¸€ä¸ªä¸“å±çš„ä»»åŠ¡æ¸…å•æ–‡ä»¶ã€‚
"""

import os
import sys
import re
import argparse
import random
from typing import List, Set
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from knowledge_distillation.process_worker import natural_sort_key

def get_target_reports(input_dir: str, num_to_process: int) -> List[str]:
    """è·å–ç›®æ ‡å¤„ç†çš„æŠ¥å‘Šæ–‡ä»¶ç»å¯¹è·¯å¾„åˆ—è¡¨ (ç»è¿‡è‡ªç„¶æ’åº)"""
    logger.info(f"æ­£åœ¨ä» {input_dir} è·å–å‰ {num_to_process} ä¸ªç›®æ ‡æŠ¥å‘Š...")
    try:
        all_files = sorted(
            [f for f in os.listdir(input_dir) if f.endswith(".txt")],
            key=natural_sort_key
        )
        target_files = all_files[:num_to_process]
        target_paths = [os.path.abspath(os.path.join(input_dir, f)) for f in target_files]
        logger.success(f"å·²ç¡®å®š {len(target_paths)} ä¸ªç›®æ ‡æ–‡ä»¶ã€‚")
        return target_paths
    except Exception as e:
        logger.error(f"è·å–ç›®æ ‡æŠ¥å‘Šå¤±è´¥: {e}")
        return []

def get_completed_reports(results_dir: str) -> Set[str]:
    """æ‰«æç»“æœç›®å½•ï¼Œè·å–æ‰€æœ‰å·²æˆåŠŸå¤„ç†çš„æŠ¥å‘Šçš„åŸºç¡€æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰"""
    logger.info(f"æ­£åœ¨ä» {results_dir} æ‰«æå·²å®Œæˆçš„å¤„ç†ç»“æœ...")
    completed_basenames: Set[str] = set()
    if not os.path.isdir(results_dir):
        logger.warning(f"ç»“æœç›®å½• {results_dir} ä¸å­˜åœ¨ï¼Œè®¤ä¸ºå·²å®Œæˆä»»åŠ¡ä¸º0ã€‚")
        return completed_basenames
        
    try:
        worker_dirs = [d for d in os.listdir(results_dir) if os.path.isdir(os.path.join(results_dir, d)) and d.startswith('worker_')]
        for worker_dir in worker_dirs:
            worker_path = os.path.join(results_dir, worker_dir)
            processed_files = [f for f in os.listdir(worker_path) if f.endswith('_processed.json')]
            for processed_file in processed_files:
                # ä» 'report_xxx_processed.json' ä¸­æå– 'report_xxx'
                basename = processed_file.replace('_processed.json', '')
                completed_basenames.add(basename)
        logger.success(f"æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(completed_basenames)} ä¸ªå·²å¤„ç†çš„æŠ¥å‘Šã€‚")
        return completed_basenames
    except Exception as e:
        logger.error(f"æ‰«æç»“æœç›®å½•å¤±è´¥: {e}")
        return completed_basenames

def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæ–­ç‚¹ç»­ä¼ çš„ä»»åŠ¡æ¸…å•æ–‡ä»¶")
    parser.add_argument("--input_dir", type=str, required=True, help="åŒ…å«åŸå§‹.txtæŠ¥å‘Šçš„ç›®å½•")
    parser.add_argument("--results_dir", type=str, required=True, help="åŒ…å«workerå­ç›®å½•çš„ç»“æœç›®å½• (e.g., result_new)")
    parser.add_argument("--task_output_dir", type=str, required=True, help="å­˜æ”¾ç”Ÿæˆçš„ä»»åŠ¡æ¸…å•æ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--total_to_process", type=int, required=True, help="åŸå§‹çš„ç›®æ ‡å¤„ç†æ€»æ•° (e.g., 10000)")
    parser.add_argument("--api_keys", nargs='+', required=True, help="è¦åˆ†é…ä»»åŠ¡çš„API keyåç§°åˆ—è¡¨ (e.g., api_1 api_4 api_5)")
    args = parser.parse_args()

    # 1. è·å–ç›®æ ‡åˆ—è¡¨å’Œå·²å®Œæˆåˆ—è¡¨
    target_paths = get_target_reports(args.input_dir, args.total_to_process)
    completed_basenames = get_completed_reports(args.results_dir)

    # 2. è®¡ç®—å¾…åŠäº‹é¡¹åˆ—è¡¨
    pending_paths = []
    for path in target_paths:
        basename = os.path.splitext(os.path.basename(path))[0]
        if basename not in completed_basenames:
            pending_paths.append(path)
    
    if not pending_paths:
        logger.success("ğŸ‰ æ­å–œï¼æ‰€æœ‰ä»»åŠ¡å‡å·²å®Œæˆï¼Œæ— éœ€ç»­ä¼ ã€‚")
        return

    logger.info(f"è®¡ç®—å®Œæˆï¼Œæœ‰ {len(pending_paths)} ä¸ªæŠ¥å‘Šéœ€è¦ç»§ç»­å¤„ç†ã€‚")

    # 3. åˆ†é…ä»»åŠ¡
    random.shuffle(pending_paths) # æ‰“ä¹±ä»»åŠ¡ï¼Œé¿å…æ‰€æœ‰å¤±è´¥ä»»åŠ¡é›†ä¸­åœ¨æŸä¸ªworker
    
    num_apis = len(args.api_keys)
    chunk_size = (len(pending_paths) + num_apis - 1) // num_apis # å‘ä¸Šå–æ•´çš„é™¤æ³•

    os.makedirs(args.task_output_dir, exist_ok=True)

    for i, api_key_name in enumerate(args.api_keys):
        start = i * chunk_size
        end = start + chunk_size
        task_chunk = pending_paths[start:end]
        
        if not task_chunk:
            continue

        task_filename = os.path.join(args.task_output_dir, f"task_{api_key_name}.txt")
        try:
            with open(task_filename, 'w', encoding='utf-8') as f:
                for path in task_chunk:
                    f.write(path + '\n')
            logger.success(f"å·²ä¸º {api_key_name} ç”Ÿæˆä»»åŠ¡æ¸…å•: {task_filename}ï¼ŒåŒ…å« {len(task_chunk)} ä¸ªä»»åŠ¡ã€‚")
        except Exception as e:
            logger.error(f"å†™å…¥ä»»åŠ¡æ¸…å• {task_filename} å¤±è´¥: {e}")

if __name__ == "__main__":
    main() 