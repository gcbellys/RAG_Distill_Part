#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†åŒ»å­¦æŠ¥å‘Šè„šæœ¬
å¤„ç†æŒ‡å®šèŒƒå›´çš„æŠ¥å‘Šï¼Œä¸ºæ¯ä¸ªæŠ¥å‘Šç”Ÿæˆrawã€normalizedã€logä¸‰ä¸ªæ–‡ä»¶
"""

import os
import sys
import json
import time
import traceback
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = "/opt/RAG_Evidence4Organ"
sys.path.insert(0, project_root)

from Question_Distillation_v2.extractors.llm_extractor import LLMExtractor
from Diag_Distillation.prompts.medical_prompts import DiagnosticExtractionPrompts
from Diag_Distillation.process_worker import process_report_with_diagnostic_steps
from configs.system_config import MULTI_API_CONFIG

class BatchReportProcessor:
    def __init__(self, api_key_name: str = "api_16"):
        self.api_key_name = api_key_name
        self.setup_api()
        self.setup_output_dir()
        self.prompts = DiagnosticExtractionPrompts()
        
    def setup_api(self):
        """åˆå§‹åŒ–API"""
        api_config = MULTI_API_CONFIG.get(self.api_key_name)
        if not api_config:
            raise ValueError(f"æœªæ‰¾åˆ°APIé…ç½®: {self.api_key_name}")
        
        self.extractor = LLMExtractor(
            model=api_config["model"],
            api_key=api_config["api_key"],
            base_url=api_config["base_url"]
        )
        print(f"âœ… APIåˆå§‹åŒ–æˆåŠŸ: {api_config['model']}")
    
    def setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_dir = "/opt/RAG_Evidence4Organ/Diag_Distillation/output_test"
        
        # æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•
        if os.path.exists(self.output_dir):
            import shutil
            shutil.rmtree(self.output_dir)
            print(f"ğŸ—‘ï¸ æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆ›å»ºæ–°çš„è¾“å‡ºç›®å½•ç»“æ„
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "raw"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "normalized"), exist_ok=True) 
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„: {self.output_dir}")
    
    def load_report(self, report_num: int) -> str:
        """åŠ è½½æŒ‡å®šç¼–å·çš„æŠ¥å‘Š"""
        report_path = f"/opt/RAG_Evidence4Organ/dataset/report_{report_num}.txt"
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            return content
        except FileNotFoundError:
            print(f"âŒ æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")
            return ""
        except Exception as e:
            print(f"âŒ è¯»å–æŠ¥å‘Šå¤±è´¥ {report_path}: {e}")
            return ""
    
    def process_single_report(self, report_num: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæŠ¥å‘Š"""
        print(f"\n{'='*20} å¤„ç†æŠ¥å‘Š {report_num} {'='*20}")
        
        # è®°å½•å¤„ç†æ—¥å¿—
        log_entries = []
        start_time = time.time()
        
        def log(message: str, level: str = "INFO"):
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            log_entry = f"[{timestamp}] [{level}] {message}"
            log_entries.append(log_entry)
            print(log_entry)
        
        log(f"å¼€å§‹å¤„ç†æŠ¥å‘Š: report_{report_num}")
        
        # åŠ è½½æŠ¥å‘Š
        report_content = self.load_report(report_num)
        if not report_content:
            log(f"æŠ¥å‘ŠåŠ è½½å¤±è´¥", "ERROR")
            return {
                "success": False,
                "error": "æŠ¥å‘ŠåŠ è½½å¤±è´¥",
                "log_entries": log_entries
            }
        
        log(f"æŠ¥å‘ŠåŠ è½½æˆåŠŸ: {len(report_content)} å­—ç¬¦")
        
        try:
            # å¤„ç†æŠ¥å‘Š
            log("å¼€å§‹è¯Šæ–­è’¸é¦å¤„ç†...")
            
            result = process_report_with_diagnostic_steps(
                extractor=self.extractor,
                report_data={"text": report_content},
                report_num=f"report_{report_num}",
                prompts=self.prompts,
                api_key_name=self.api_key_name
            )
            
            processing_time = time.time() - start_time
            
            if result:
                # åˆ†æç»“æœ
                normalized = result.get("normalized", [])
                raw = result.get("raw", {})
                
                log(f"å¤„ç†æˆåŠŸå®Œæˆ (è€—æ—¶: {processing_time:.1f}ç§’)")
                
                # å®‰å…¨æ£€æŸ¥: ç¡®ä¿normalizedæ˜¯åˆ—è¡¨ç±»å‹
                if isinstance(normalized, list):
                    log(f"æ ‡å‡†åŒ–ç»“æœ: {len(normalized)} ä¸ªæ¡ç›®")
                    
                    if normalized:
                        total_units = sum(len(item.get('U_unit_set', [])) for item in normalized if isinstance(item, dict))
                        unique_organs = set()
                        for item in normalized:
                            if isinstance(item, dict):
                                for unit_wrapper in item.get('U_unit_set', []):
                                    if isinstance(unit_wrapper, dict):
                                        u_unit = unit_wrapper.get('u_unit', {})
                                        if isinstance(u_unit, dict):
                                            organ_name = u_unit.get('o_organ', {}).get('organName')
                                            if organ_name:
                                                unique_organs.add(organ_name)
                        
                        log(f"è¯Šæ–­å•å…ƒæ€»æ•°: {total_units}")
                        log(f"æ¶‰åŠå™¨å®˜æ•°: {len(unique_organs)}")
                        if unique_organs:
                            log(f"æ¶‰åŠå™¨å®˜: {', '.join(sorted(unique_organs))}")
                else:
                    log(f"è­¦å‘Š: æ ‡å‡†åŒ–ç»“æœç±»å‹å¼‚å¸¸: {type(normalized)}")
                    log(f"æ ‡å‡†åŒ–ç»“æœå†…å®¹: {normalized}")
                
                # ä¿å­˜ç»“æœæ–‡ä»¶
                self.save_results(report_num, result)
                
                return {
                    "success": True,
                    "report_num": report_num,
                    "raw": raw,
                    "normalized": normalized,
                    "processing_time": processing_time,
                    "log_entries": log_entries,
                    "statistics": {
                        "total_findings": len(normalized),
                        "total_units": sum(len(item.get('U_unit_set', [])) for item in normalized),
                        "unique_organs": len(unique_organs) if 'unique_organs' in locals() else 0
                    }
                }
            else:
                log("å¤„ç†å¤±è´¥: æœªè¿”å›ç»“æœ", "ERROR")
                return {
                    "success": False,
                    "error": "å¤„ç†å¤±è´¥: æœªè¿”å›ç»“æœ",
                    "log_entries": log_entries
                }
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}"
            log(error_msg, "ERROR")
            log(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}", "ERROR")
            
            return {
                "success": False,
                "error": error_msg,
                "processing_time": processing_time,
                "log_entries": log_entries
            }
    
    def save_results(self, report_num: int, result: Dict[str, Any]):
        """ä¿å­˜å¤„ç†ç»“æœåˆ°ä¸‰ä¸ªæ–‡ä»¶"""
        
        # æ–‡ä»¶è·¯å¾„
        raw_file = os.path.join(self.output_dir, "raw", f"report_{report_num}_raw.json")
        normalized_file = os.path.join(self.output_dir, "normalized", f"report_{report_num}_normalized.json")
        log_file = os.path.join(self.output_dir, "logs", f"report_{report_num}_log.txt")
        
        try:
            if result["success"]:
                # ä¿å­˜rawç»“æœ
                with open(raw_file, 'w', encoding='utf-8') as f:
                    json.dump(result["raw"], f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜normalizedç»“æœ
                with open(normalized_file, 'w', encoding='utf-8') as f:
                    json.dump(result["normalized"], f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜:")
                print(f"   Raw: {raw_file}")
                print(f"   Normalized: {normalized_file}")
            else:
                # ä¿å­˜é”™è¯¯ä¿¡æ¯
                error_info = {
                    "success": False,
                    "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                    "timestamp": datetime.now().isoformat()
                }
                
                with open(raw_file, 'w', encoding='utf-8') as f:
                    json.dump(error_info, f, ensure_ascii=False, indent=2)
                
                with open(normalized_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                
                print(f"âŒ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜:")
                print(f"   Raw: {raw_file}")
                print(f"   Normalized: {normalized_file}")
            
            # ä¿å­˜æ—¥å¿—
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(result["log_entries"]))
            
            print(f"   Log: {log_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def process_batch(self, start_num: int, end_num: int):
        """æ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æŠ¥å‘Š")
        print(f"ğŸ“Š å¤„ç†èŒƒå›´: report_{start_num} - report_{end_num}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”‘ APIé…ç½®: {self.api_key_name}")
        print()
        
        total_reports = end_num - start_num + 1
        successful = 0
        failed = 0
        batch_start_time = time.time()
        
        for report_num in range(start_num, end_num + 1):
            try:
                # å¤„ç†æŠ¥å‘Š
                result = self.process_single_report(report_num)
                
                # ä¿å­˜ç»“æœ
                self.save_results(report_num, result)
                
                if result["success"]:
                    successful += 1
                    print(f"âœ… report_{report_num}: å¤„ç†æˆåŠŸ")
                else:
                    failed += 1
                    print(f"âŒ report_{report_num}: å¤„ç†å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # è¿›åº¦æ˜¾ç¤º
                progress = ((report_num - start_num + 1) / total_reports) * 100
                print(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({report_num - start_num + 1}/{total_reports})")
                
                # ç®€çŸ­ä¼‘æ¯é¿å…APIé™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                failed += 1
                print(f"âŒ report_{report_num}: ä¸¥é‡é”™è¯¯ - {e}")
                
                # è®°å½•é”™è¯¯å¹¶ç»§ç»­
                error_result = {
                    "success": False,
                    "error": str(e),
                    "log_entries": [f"[{datetime.now()}] [ERROR] ä¸¥é‡é”™è¯¯: {e}"]
                }
                self.save_results(report_num, error_result)
        
        # æ‰¹é‡å¤„ç†æ€»ç»“
        total_time = time.time() - batch_start_time
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆæ€»ç»“")
        print(f"{'='*60}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æŠ¥å‘Šæ•°: {total_reports}")
        print(f"   æˆåŠŸå¤„ç†: {successful}")
        print(f"   å¤„ç†å¤±è´¥: {failed}")
        print(f"   æˆåŠŸç‡: {(successful/total_reports)*100:.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"âš¡ å¹³å‡æ¯æŠ¥å‘Š: {total_time/total_reports:.1f}ç§’")
        
        # ç”Ÿæˆæ‰¹é‡å¤„ç†æ‘˜è¦
        summary = {
            "batch_info": {
                "start_num": start_num,
                "end_num": end_num,
                "total_reports": total_reports,
                "successful": successful,
                "failed": failed,
                "success_rate": f"{(successful/total_reports)*100:.1f}%",
                "total_time": f"{total_time:.1f}s",
                "avg_time_per_report": f"{total_time/total_reports:.1f}s"
            },
            "timestamp": datetime.now().isoformat(),
            "api_config": self.api_key_name,
            "output_directory": self.output_dir
        }
        
        summary_file = os.path.join(self.output_dir, "batch_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ æ‰¹é‡æ‘˜è¦å·²ä¿å­˜: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»å­¦æŠ¥å‘Šæ‰¹é‡å¤„ç†ç³»ç»Ÿ")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = BatchReportProcessor(api_key_name="api_16")
        
        # æ‰¹é‡å¤„ç†æŠ¥å‘Š10061-10070
        processor.process_batch(start_num=10061, end_num=10070)
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main() 