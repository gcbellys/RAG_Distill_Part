#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œæ‰¹é‡å¤„ç†åŒ»å­¦æŠ¥å‘Šè„šæœ¬
ä½¿ç”¨å¤šä¸ªAPIå¯†é’¥å¹¶è¡Œå¤„ç†æŒ‡å®šèŒƒå›´çš„æŠ¥å‘Š
"""

import os
import sys
import json
import time
import traceback
import threading
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = "/opt/RAG_Evidence4Organ"
sys.path.insert(0, project_root)

from Question_Distillation_v2.extractors.llm_extractor import LLMExtractor
from Diag_Distillation.prompts.medical_prompts import DiagnosticExtractionPrompts
from Diag_Distillation.process_worker import process_report_with_diagnostic_steps
from configs.system_config import MULTI_API_CONFIG

class ParallelBatchReportProcessor:
    def __init__(self, api_keys: List[str] = None):
        if api_keys is None:
            api_keys = ["api_13", "api_14", "api_15", "api_16", "api_12"]
        
        self.api_keys = api_keys
        self.setup_apis()
        self.setup_output_dir()
        self.prompts = DiagnosticExtractionPrompts()
        
        # çº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨
        self.results_lock = threading.Lock()
        self.processing_results = {}
        
    def setup_apis(self):
        """åˆå§‹åŒ–æ‰€æœ‰API"""
        self.extractors = {}
        for api_key in self.api_keys:
            try:
                if api_key not in MULTI_API_CONFIG:
                    print(f"âŒ APIå¯†é’¥ {api_key} ä¸å­˜åœ¨äºé…ç½®ä¸­")
                    continue
                    
                api_config = MULTI_API_CONFIG[api_key]
                extractor = LLMExtractor(
                    model=api_config["model"],
                    api_key=api_config["api_key"],
                    base_url=api_config["base_url"]
                )
                
                self.extractors[api_key] = extractor
                print(f"âœ… APIåˆå§‹åŒ–æˆåŠŸ: {api_key} - {api_config['model']}")
                
            except Exception as e:
                print(f"âŒ API {api_key} åˆå§‹åŒ–å¤±è´¥: {e}")
        
        if not self.extractors:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„API")
            
        print(f"ğŸ”— æˆåŠŸåˆå§‹åŒ– {len(self.extractors)} ä¸ªAPI")
        
    def setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_dir = "/opt/RAG_Evidence4Organ/Diag_Distillation/output_test"
        
        # æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•
        if os.path.exists(self.output_dir):
            import shutil
            shutil.rmtree(self.output_dir)
            print(f"ğŸ—‘ï¸ æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆ›å»ºæ–°çš„è¾“å‡ºç›®å½•ç»“æ„
        os.makedirs(f"{self.output_dir}/raw", exist_ok=True)
        os.makedirs(f"{self.output_dir}/normalized", exist_ok=True)
        os.makedirs(f"{self.output_dir}/logs", exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„: {self.output_dir}")
        
    def load_report(self, report_num: int) -> str:
        """åŠ è½½æŠ¥å‘Šæ•°æ®"""
        dataset_dir = "/opt/RAG_Evidence4Organ/dataset"
        
        # å°è¯•åŠ è½½txtæ–‡ä»¶
        txt_file = os.path.join(dataset_dir, f"report_{report_num}.txt")
        if os.path.exists(txt_file):
            with open(txt_file, 'r', encoding='utf-8') as f:
                return f.read()
        
        # å°è¯•åŠ è½½jsonæ–‡ä»¶
        json_file = os.path.join(dataset_dir, f"report_{report_num}.json")
        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('text', '') or data.get('medical_record_content', '')
        
        raise FileNotFoundError(f"æŠ¥å‘Š {report_num} ä¸å­˜åœ¨ (txtæˆ–json)")
    
    def process_single_report(self, report_num: int, api_key: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæŠ¥å‘Š"""
        
        def log(message: str):
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_message = f"[{timestamp}] [INFO] {message}"
            print(f"[{api_key}] {log_message}")
            return log_message
        
        try:
            start_time = time.time()
            log(f"å¼€å§‹å¤„ç†æŠ¥å‘Š: report_{report_num}")
            
            # åŠ è½½æŠ¥å‘Š
            report_text = self.load_report(report_num)
            log(f"æŠ¥å‘ŠåŠ è½½æˆåŠŸ: {len(report_text)} å­—ç¬¦")
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report_data = {
                'text': report_text,
                'case_id': str(report_num),
                'filename': f'report_{report_num}.txt'
            }
            
            log("å¼€å§‹è¯Šæ–­è’¸é¦å¤„ç†...")
            
            # å¤„ç†æŠ¥å‘Š
            extractor = self.extractors[api_key]
            result = process_report_with_diagnostic_steps(
                extractor, report_data, report_num, self.prompts, api_key
            )
            
            if not result:
                raise Exception("å¤„ç†å¤±è´¥ï¼šæ— è¿”å›ç»“æœ")
            
            processing_time = time.time() - start_time
            
            # å®‰å…¨è·å–æ•°æ®
            normalized = result.get("normalized", [])
            raw = result.get("raw", {})
            
            log(f"å¤„ç†æˆåŠŸå®Œæˆ (è€—æ—¶: {processing_time:.1f}ç§’)")
            
            # å®‰å…¨æ£€æŸ¥: ç¡®ä¿normalizedæ˜¯åˆ—è¡¨ç±»å‹
            if isinstance(normalized, list):
                log(f"æ ‡å‡†åŒ–ç»“æœ: {len(normalized)} ä¸ªæ¡ç›®")
                
                if normalized:
                    total_units = sum(len(item.get('U_unit_set', [])) for item in normalized if isinstance(item, dict))
                    unique_organs = set()
                    for item in normalized:
                        if isinstance(item, dict):
                            for unit_wrapper in item.get('U_unit_set', []):
                                if isinstance(unit_wrapper, dict):
                                    u_unit = unit_wrapper.get('u_unit', {})
                                    if isinstance(u_unit, dict):
                                        organ_name = u_unit.get('o_organ', {}).get('organName')
                                        if organ_name:
                                            unique_organs.add(organ_name)
                    
                    log(f"è¯Šæ–­å•å…ƒæ€»æ•°: {total_units}")
                    log(f"æ¶‰åŠå™¨å®˜æ•°: {len(unique_organs)}")
                    if unique_organs:
                        log(f"æ¶‰åŠå™¨å®˜: {', '.join(sorted(unique_organs))}")
            else:
                log(f"è­¦å‘Š: æ ‡å‡†åŒ–ç»“æœç±»å‹å¼‚å¸¸: {type(normalized)}")
                log(f"æ ‡å‡†åŒ–ç»“æœå†…å®¹: {normalized}")
            
            # ä¿å­˜ç»“æœæ–‡ä»¶
            self.save_results(report_num, result, api_key)
            
            return {
                "success": True,
                "report_num": report_num,
                "api_key": api_key,
                "processing_time": processing_time,
                "total_units": sum(len(item.get('U_unit_set', [])) for item in normalized if isinstance(item, dict) and isinstance(normalized, list)),
                "unique_organs": list(unique_organs) if isinstance(normalized, list) else [],
                "log_messages": []
            }
            
        except Exception as e:
            error_msg = f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}"
            log(f"ERROR] {error_msg}")
            log(f"ERROR] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            error_result = {
                "success": False,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }
            self.save_results(report_num, {"raw": error_result, "normalized": []}, api_key)
            
            return {
                "success": False,
                "report_num": report_num,
                "api_key": api_key,
                "error": error_msg,
                "log_messages": []
            }
    
    def save_results(self, report_num: int, result: Dict[str, Any], api_key: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        try:
            # ä¿å­˜åŸå§‹ç»“æœ
            raw_file = f"{self.output_dir}/raw/report_{report_num}_raw.json"
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(result.get("raw", {}), f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ ‡å‡†åŒ–ç»“æœ
            normalized_file = f"{self.output_dir}/normalized/report_{report_num}_normalized.json"
            with open(normalized_file, 'w', encoding='utf-8') as f:
                json.dump(result.get("normalized", []), f, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºç®€å•çš„æ—¥å¿—æ–‡ä»¶
            log_file = f"{self.output_dir}/logs/report_{report_num}_log.txt"
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"Report: {report_num}\n")
                f.write(f"API: {api_key}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                if result.get("raw", {}).get("success", True):
                    f.write("Status: Success\n")
                else:
                    f.write("Status: Failed\n")
                    f.write(f"Error: {result.get('raw', {}).get('error', 'Unknown')}\n")
            
            print(f"[{api_key}] ğŸ’¾ ç»“æœå·²ä¿å­˜:")
            print(f"[{api_key}]    Raw: {raw_file}")
            print(f"[{api_key}]    Normalized: {normalized_file}")
            print(f"[{api_key}]    Log: {log_file}")
            
        except Exception as e:
            print(f"[{api_key}] âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def process_batch_parallel(self, start_num: int, end_num: int, max_workers: int = None):
        """å¹¶è¡Œæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        if max_workers is None:
            max_workers = len(self.extractors)
        
        print("ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç†æŠ¥å‘Š")
        print("=" * 80)
        print(f"ğŸ“Š å¤„ç†èŒƒå›´: report_{start_num} - report_{end_num}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”— å¯ç”¨API: {list(self.extractors.keys())}")
        print(f"ğŸ§µ æœ€å¤§å¹¶å‘æ•°: {max_workers}")
        print()
        
        # å‡†å¤‡æŠ¥å‘Šä»»åŠ¡
        report_nums = list(range(start_num, end_num + 1))
        api_queue = Queue()
        
        # å°†APIå¯†é’¥æ”¾å…¥é˜Ÿåˆ—ï¼Œå®ç°è´Ÿè½½å‡è¡¡
        for api_key in self.extractors.keys():
            api_queue.put(api_key)
        
        start_time = time.time()
        completed_count = 0
        success_count = 0
        failed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ›å»ºä»»åŠ¡æ˜ å°„
            future_to_report = {}
            
            for report_num in report_nums:
                # ä»é˜Ÿåˆ—ä¸­è·å–APIå¯†é’¥
                if not api_queue.empty():
                    api_key = api_queue.get()
                else:
                    # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªAPI
                    api_key = list(self.extractors.keys())[0]
                
                future = executor.submit(self.process_single_report, report_num, api_key)
                future_to_report[future] = (report_num, api_key)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_report):
                report_num, api_key = future_to_report[future]
                
                try:
                    result = future.result()
                    completed_count += 1
                    
                    if result["success"]:
                        success_count += 1
                        print(f"âœ… [{api_key}] report_{report_num}: å¤„ç†æˆåŠŸ")
                    else:
                        failed_count += 1
                        print(f"âŒ [{api_key}] report_{report_num}: å¤„ç†å¤±è´¥ - {result.get('error', 'Unknown')}")
                    
                    # å°†APIå¯†é’¥æ”¾å›é˜Ÿåˆ—
                    api_queue.put(api_key)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = (completed_count / len(report_nums)) * 100
                    print(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({completed_count}/{len(report_nums)})")
                    print()
                    
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ [{api_key}] report_{report_num}: æ‰§è¡Œå¼‚å¸¸ - {e}")
                    # å°†APIå¯†é’¥æ”¾å›é˜Ÿåˆ—
                    api_queue.put(api_key)
        
        # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
        total_time = time.time() - start_time
        self.generate_batch_summary(start_num, end_num, len(report_nums), success_count, failed_count, total_time)
    
    def generate_batch_summary(self, start_num: int, end_num: int, total: int, success: int, failed: int, total_time: float):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æ€»ç»“"""
        print()
        print("=" * 60)
        print("ğŸ“Š å¹¶è¡Œæ‰¹é‡å¤„ç†å®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æŠ¥å‘Šæ•°: {total}")
        print(f"   æˆåŠŸå¤„ç†: {success}")
        print(f"   å¤„ç†å¤±è´¥: {failed}")
        print(f"   æˆåŠŸç‡: {(success/total*100):.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"âš¡ å¹³å‡æ¯æŠ¥å‘Š: {total_time/total:.1f}ç§’")
        
        # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶
        summary = {
            "batch_info": {
                "start_num": start_num,
                "end_num": end_num,
                "total_reports": total,
                "successful": success,
                "failed": failed,
                "success_rate": f"{(success/total*100):.1f}%",
                "total_time": f"{total_time:.1f}s",
                "avg_time_per_report": f"{total_time/total:.1f}s"
            },
            "timestamp": datetime.now().isoformat(),
            "api_config": list(self.extractors.keys()),
            "output_directory": self.output_dir
        }
        
        summary_file = f"{self.output_dir}/batch_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ æ‰¹é‡æ‘˜è¦å·²ä¿å­˜: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŒ»å­¦æŠ¥å‘Šå¹¶è¡Œæ‰¹é‡å¤„ç†ç³»ç»Ÿ")
    print("=" * 80)
    
    try:
        # ä½¿ç”¨5ä¸ªAPIå¯†é’¥è¿›è¡Œå¹¶è¡Œå¤„ç†
        api_keys = ["api_13", "api_14", "api_15", "api_16", "api_12"]
        processor = ParallelBatchReportProcessor(api_keys=api_keys)
        
        # å¹¶è¡Œæ‰¹é‡å¤„ç†æŠ¥å‘Š10061-10070
        processor.process_batch_parallel(start_num=10061, end_num=10070, max_workers=5)
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main() 