#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG ç³»ç»Ÿ vs ç‹¬ç«‹LLM å¯¹æ¯”æµ‹è¯•è„šæœ¬ (å‡çº§ç‰ˆ)
ä½œè€…: Gemini & CDJ_LP
æè¿°:
è¯¥è„šæœ¬æ¥æ”¶ä¸€ä¸ªè‹±æ–‡åŒ»å­¦é—®é¢˜ï¼Œå¹¶è¡Œè·å–RAGç­”æ¡ˆå’Œå¤šä¸ªç‹¬ç«‹LLMçš„ç­”æ¡ˆï¼Œ
å¹¶å°†æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼ˆé—®é¢˜ã€promptã€RAGç»“æœã€å„LLMç»“æœï¼‰
å®Œæ•´åœ°ä¿å­˜åˆ°ä¸€ä¸ªç»“æ„åŒ–çš„JSONæ–‡ä»¶ä¸­ï¼Œä»¥ä¾¿äºåˆ†æå’Œè¯„ä¼°ã€‚

æ‰§è¡Œæ–¹æ³•:
python RAG_Evidence4Organ/llm_evaluation/run_rag_vs_llm_test.py --question "A sample English medical question"
"""

import os
import sys
import json
import argparse
import time
from pathlib import Path
from typing import Dict, Any, List

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

from loguru import logger
from RAG_Evidence4Organ.configs.system_config import MULTI_API_CONFIG, RAG_CONFIG
from RAG_Evidence4Organ.configs.evaluation_llm_config import EVAL_LLM_CONFIG
from RAG_Evidence4Organ.configs.model_config import ORGAN_ANATOMY_STRUCTURE
from RAG_Evidence4Organ.knowledge_distillation.extractors.llm_extractor import LLMExtractor
# from RAG_Evidence4Organ.rag_system.query_handler import RAGQueryHandler
# from RAG_Evidence4Organ.rag_system.embedding_loader import EmbeddingLoader
# from RAG_Evidence4Organ.rag_system.vector_store_handler import VectorStoreHandler
# from RAG_Evidence4Organ.rag_system.llm_qa_handler import LLMQAHandler
# å¯¼å…¥æ–°çš„RAGæœåŠ¡
from RAG_Evidence4Organ.rag_system.api.main import RAGService

# def get_rag_answer(question: str, query_handler: RAGQueryHandler) -> Dict[str, Any]:
#     """é€šè¿‡RAGç³»ç»Ÿè·å–ç­”æ¡ˆ"""
#     logger.info("... æ­£åœ¨é€šè¿‡RAGç³»ç»ŸæŸ¥è¯¢ ...")
#     try:
#         result = query_handler.handle_query(question)
#         logger.success("âœ… RAGç³»ç»ŸæŸ¥è¯¢æˆåŠŸã€‚")
#         return result
#     except Exception as e:
#         logger.error(f"âŒ RAGç³»ç»ŸæŸ¥è¯¢å¤±è´¥: {e}")
#         return {"error": str(e)}

def get_structured_prompt_template() -> str:
    """æ„å»ºåŒ…å«è§„èŒƒåŒ–å™¨å®˜ç»“æ„çš„ä¸¥æ ¼Promptæ¨¡æ¿ã€‚"""
    # è½¬ä¹‰JSONä¸­çš„èŠ±æ‹¬å·ï¼Œä»¥ä¾¿å®ƒä»¬åœ¨.format()ä¸­è¢«å½“ä½œæ–‡å­—å¤„ç†
    structure_json_escaped = json.dumps(ORGAN_ANATOMY_STRUCTURE, indent=2).replace("{", "{{").replace("}", "}}")
    
    system_prompt_template = f"""You are an expert radiologist. Your task is to analyze a clinical query based on the provided context and determine which organ and specific anatomical structures should be examined.

--- Retrieved Knowledge ---
{{context_docs}}
--- End Retrieved Knowledge ---

You MUST follow these rules:
1.  First, provide a brief "Symptom Analysis".
2.  Second, identify the primary "Organ" to be examined from the keys in the provided JSON structure.
3.  Third, from the list of anatomical structures corresponding to that organ in the JSON, select the specific "Anatomical Structures" that require examination.
4.  Your final output MUST be a single JSON object with three keys: "Symptom Analysis", "Organ", and "Anatomical Structures". The value for "Anatomical Structures" must be a list of strings.
5.  The organ and structure names you choose MUST EXACTLY match the names provided in the JSON structure below. Do not invent new names or use synonyms.

Here is the authoritative list of organs and their anatomical structures:
{structure_json_escaped}

--- Clinical Query ---
{{query}}
--- End Clinical Query ---

Please provide your analysis in the specified JSON format.
"""
    return system_prompt_template

def get_rag_answer(question: str, rag_service: RAGService, structured_prompt_template: str) -> Dict[str, Any]:
    """é€šè¿‡RAGç³»ç»Ÿè·å–ç­”æ¡ˆ"""
    logger.info("... æ­£åœ¨é€šè¿‡RAGç³»ç»ŸæŸ¥è¯¢ ...")
    try:
        # ä½¿ç”¨æ–°çš„æœåŠ¡æ¥å¤„ç†æŸ¥è¯¢, å¹¶ä¼ å…¥æ–°çš„promptæ¨¡æ¿
        result = rag_service.answer_query(question, custom_qa_prompt_template=structured_prompt_template)
        logger.success("âœ… RAGç³»ç»ŸæŸ¥è¯¢æˆåŠŸã€‚")
        return result
    except Exception as e:
        logger.error(f"âŒ RAGç³»ç»ŸæŸ¥è¯¢å¤±è´¥: {e}")
        return {"error": str(e)}

def get_direct_llm_answers(question: str, llm_configs: Dict[str, Any], structured_prompt_template: str) -> List[Dict[str, Any]]:
    """ç›´æ¥ä»å¤šä¸ªLLMè·å–ç­”æ¡ˆï¼Œå¹¶åŒ…å«prompt"""
    logger.info("... æ­£åœ¨ç›´æ¥å‘å¤šä¸ªLLMæŸ¥è¯¢ ...")
    direct_answers = []
    # ä¸ºç›´æ¥è°ƒç”¨LLMæ ¼å¼åŒ–promptï¼Œä¸æä¾›ä¸Šä¸‹æ–‡
    direct_llm_prompt = structured_prompt_template.format(
        context_docs="N/A. Please rely on your general knowledge.",
        query=question
    )

    for api_name, config in llm_configs.items():
        logger.info(f"  - æ­£åœ¨æŸ¥è¯¢API: '{api_name}' ({config.get('model')})...")
        api_result = {
            "api_name": api_name,
            "model": config.get('model', 'N/A'),
            "answer": None,
            "prompt": direct_llm_prompt, # ä¿å­˜æœ€ç»ˆå‘é€çš„å®Œæ•´prompt
            "error": None
        }
        try:
            extractor = LLMExtractor(
                model=config["model"],
                api_key=config["api_key"],
                base_url=config["base_url"]
            )
            
            result = extractor.call_api(direct_llm_prompt)

            if result["success"]:
                api_result["answer"] = result["response"]
            else:
                api_result["error"] = result["error"]
        except Exception as e:
            api_result["error"] = str(e)
            logger.error(f"  - æŸ¥è¯¢API '{api_name}' å¤±è´¥: {e}")
        
        direct_answers.append(api_result)

    logger.success("âœ… æ‰€æœ‰ç‹¬ç«‹LLMæŸ¥è¯¢å®Œæˆã€‚")
    return direct_answers

def save_results_to_json(output_dir: Path, data: Dict[str, Any]):
    """å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}.json"
    
    file_path = output_dir / filename
    
    logger.info(f"ğŸ’¾ æ­£åœ¨å°†æµ‹è¯•ç»“æœä¿å­˜åˆ°: {file_path}")
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.success("âœ… ç»“æœæ–‡ä»¶ä¿å­˜æˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿ vs ç‹¬ç«‹LLM å¯¹æ¯”æµ‹è¯•å·¥å…· (å‡çº§ç‰ˆ)")
    parser.add_argument("--question", type=str, required=True, help="è¦æµ‹è¯•çš„è‹±æ–‡åŒ»å­¦é—®é¢˜")
    args = parser.parse_args()

    logger.info("ğŸš€ å¯åŠ¨å¯¹æ¯”æµ‹è¯•æµç¨‹...")
    
    # æœ€ç»ˆç»“æœçš„è¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / "RAG_output"

    # å‡†å¤‡æ–°çš„ã€ä¸¥æ ¼çš„promptæ¨¡æ¿
    structured_prompt_template = get_structured_prompt_template()

    # 1. åˆå§‹åŒ–RAGæœåŠ¡
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–RAGæœåŠ¡...")
        rag_service = RAGService()
        logger.success("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæ¯•ã€‚")
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–RAGæœåŠ¡å¤±è´¥: {e}")
        return

    # 2. è·å–ç­”æ¡ˆ
    rag_answer = get_rag_answer(args.question, rag_service, structured_prompt_template)
    direct_llm_answers = get_direct_llm_answers(args.question, EVAL_LLM_CONFIG, structured_prompt_template)
    
    # 3. å‡†å¤‡è¦ä¿å­˜çš„å®Œæ•´æ•°æ®ç»“æ„
    final_result = {
        "question": args.question,
        "rag_result": rag_answer,
        "direct_llm_results": direct_llm_answers
    }
    
    # 4. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    save_results_to_json(output_dir, final_result)
    
    logger.success("ğŸ‰ å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main() 